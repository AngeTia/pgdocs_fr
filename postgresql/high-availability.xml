<?xml version="1.0" encoding="ISO-8859-15"?>
<!-- Dernière modification
     le       $Date$
     par      $Author$
     révision $Revision$ -->

<chapter id="high-availability">
 <title>Haute disponibilité, répartition de charge et réplication</title>

 <indexterm><primary>haute disponibilité</primary></indexterm>
 <indexterm><primary>failover</primary></indexterm>
 <indexterm><primary>réplication</primary></indexterm>
 <indexterm><primary>répartition de charge</primary></indexterm>
 <indexterm><primary>clustering</primary></indexterm>
 <indexterm><primary>partitionnement de données</primary></indexterm>

<!-- seamlessly ? -->
 <para>
  Des serveurs de bases de données peuvent travailler ensemble pour permettre
  à un serveur secondaire de prendre rapidement la main si le serveur principal
  échoue (haute disponibilité, ou <foreignphrase>high availability</foreignphrase>),
  ou pour permettre à plusieurs serveurs de servir les mêmes données (répartition
  de charge, ou <foreignphrase>load balancing</foreignphrase>). Idéalement, les
  serveurs de bases de données peuvent travailler ensemble sans jointure.
 </para>
  
 <para>
  Il est aisé de faire coopérer des serveurs web qui traitent des pages web statiques
  en répartissant la charge des requêtes web sur plusieurs
  machines. Dans les faits, les serveurs de bases de données en lecture seule peuvent
  également coopérer facilement. Malheureusement, la plupart des
  serveurs de bases de données traitent des requêtes de lecture/écriture et,
  de ce fait, collaborent plus difficilement. En effet, alors qu'il suffit de
  placer une seule fois les données en lecture seule sur chaque serveur, une
  écriture sur n'importe quel serveur doit, elle, être propagée à tous les
  serveurs afin que les lectures suivantes sur ces serveurs renvoient des résultats
  cohérents.
 </para>

 <para>
  Ce problème de synchronisation représente la difficulté fondamentale à la
  collaboration entre serveurs. Comme la solution au problème de
  synchronisation n'est pas unique pour tous les cas pratiques, plusieurs
  solutions co-existent. Chacune répond de façon différente et minimise
  cet impact au regard d'une charge spécifique.
 </para>

 <para>
  Certaines solutions gèrent la synchronisation en autorisant les modifications
  des données sur un seul serveur. Les serveurs qui peuvent modifier les données
  sont appelés serveur en lecture/écriture, <firstterm>maître</firstterm> ou serveur <firstterm>primaire</firstterm>.
  Les serveurs qui suivent les modifications du maître sont appelés <firstterm>standby</firstterm>,
  ou serveurs <firstterm>esclaves</firstterm>. Un serveur en standby auquel on ne peut pas
  se connecter tant qu'il n'a pas été promu en serveur maître est appelé un serveur en <firstterm>warm
  standby</firstterm>, et un qui peut accepter des connections et répondre à des requêtes en
  lecture seule est appelé un serveur en <firstterm>hot standby</firstterm>.
 </para>

 <para>
  Certaines solutions sont synchrones, ce qui signifie qu'une transaction de
  modification de données n'est pas considérée valide tant que tous les
  serveurs n'ont pas validé la transaction. Ceci garantit qu'un
  <foreignphrase>failover</foreignphrase> ne perd pas de données et que tous
  les serveurs en répartition de charge retournent des résultats cohérents, quel
  que soit le serveur interrogé. Au contraire, les solutions asynchrones
  autorisent un délai entre la validation et sa propagation aux
  autres serveurs. Cette solution implique une éventuelle perte de transactions
  lors de la bascule sur un serveur de sauvegarde, ou l'envoi de données
  obsolètes par les serveurs à charge répartie. La communication asynchrone est
  utilisée lorsque la version synchrone est trop lente.
 </para>

 <para>
  Les solutions peuvent aussi être catégorisées par leur granularité. Certaines
  ne gèrent que la totalité d'un serveur de bases alors que
  d'autres autorisent un contrôle par table ou par base.
 </para>

 <para>
  Il importe de considérer les performances dans tout choix. Il y
  a généralement un compromis à trouver entre les fonctionnalités et les
  performances. Par exemple, une solution complètement synchrone sur un réseau
  lent peut diviser les performances par plus de deux, alors qu'une
  solution asynchrone peut n'avoir qu'un impact minimal sur les performances.
 </para>

 <para>
  Le reste de cette section souligne différentes solutions de
  <foreignphrase>failover</foreignphrase>, de réplication et de répartition de
  charge. Un <ulink
  url="http://www.postgres-r.org/documentation/terms">glossaire</ulink> est
  aussi disponible.
 </para>

 <sect1 id="different-replication-solutions">
 <title>Comparaison de différentes solutions</title>

 <variablelist>

 <varlistentry>
  <term><foreignphrase>Failover</foreignphrase> sur disque partagé</term>
  <listitem>

   <para>
    Le <foreignphrase>failover</foreignphrase> (ou bascule sur incident)
    sur disque partagé élimine la surcharge de synchronisation par
    l'existence d'une seule copie de la base de données. Il utilise un
    seul ensemble de disques partagé par plusieurs serveurs. Si le serveur
    principal échoue, le serveur en attente
    est capable de monter et démarrer la base comme s'il récupérait d'un
    arrêt brutal. Cela permet un <foreignphrase>failover</foreignphrase>
    rapide sans perte de données.
   </para>

   <para>
    La fonctionnalité de matériel partagé est commune aux périphériques de
    stockage en réseau. Il est également possible d'utiliser un système de
    fichiers réseau bien qu'il faille porter une grande attention au système de
    fichiers pour s'assurer qu'il a un comportement <acronym>POSIX</acronym>
    complet (voir <xref linkend="creating-cluster-nfs"/>). Cette méthode
    comporte une limitation significative&nbsp;: si les disques ont un
    problème ou sont corrompus, le serveur primaire et le serveur en attente sont tous
    les deux non fonctionnels. Un autre problème est que le serveur en attente
    ne devra jamais accéder au stockage partagé tant que le serveur principal
    est en cours d'exécution.
   </para>

   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Réplication de système de fichiers (périphérique bloc)</term>
   <listitem>

   <para>
    Il est aussi possible d'utiliser cette fonctionnalité d'une autre façon
    avec une réplication du système de fichiers, où toutes les modifications
    d'un système de fichiers sont renvoyées sur un système de fichiers situé
    sur un autre ordinateur. La seule restriction est que ce miroir doit être
    construit de telle sorte que le serveur en attente dispose d'une
    version cohérente du système de fichiers &mdash; spécifiquement, les
    écritures sur le serveur en attente doivent être réalisées dans le même
    ordre que celles sur le maître. <productname>DRBD</productname> est une
    solution populaire de réplication de systèmes de fichiers pour Linux.
   </para>

<!--
https://forge.continuent.org/pipermail/sequoia/2006-November/004070.html

Oracle RAC is a shared disk approach and just send cache invalidations
to other nodes but not actual data. As the disk is shared, data is
only committed once to disk and there is a distributed locking
protocol to make nodes agree on a serializable transactional order.
-->

  </listitem>
 </varlistentry>

 <varlistentry>
  <term><foreignphrase>Warm et Hot Standby</foreignphrase> en utilisant
    <acronym>PITR</acronym></term>
  <listitem>

   <para>
    Les serveurs <foreignphrase>warm et hot standby</foreignphrase> (voir <xref
    linkend="warm-standby"/>) peuvent conserver leur cohérence en lisant un flux
    d'enregistrements de <acronym>WAL</acronym>. Si le serveur principal
    échoue, le serveur
    <foreignphrase>standby</foreignphrase> contient pratiquement toutes
    les données du serveur principal et peut rapidement devenir le nouveau
    serveur maître. Ceci est asynchrone et ne peut se faire que pour le
    serveur de bases complet.
   </para>
    <para>
     Un serveur de standby en PITR peut être implémenté en utilisant la recopie de journaux par fichier
     (<xref linkend="warm-standby"/>)  ou la streaming replication (réplication en continu, voir 
     <xref linkend="streaming-replication"/>), ou une combinaison des deux. Pour
     des informations sur le hot standby, voyez <xref linkend="hot-standby"/>..
    </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Réplication maître/esclave basé sur des triggers</term>
  <listitem>

   <para>
    Une configuration de réplication maître/esclave envoie toutes les requêtes
    de modification de données au serveur maître. Ce serveur envoie les
    modifications de données de façon asynchrone au serveur esclave. L'esclave
    peut répondre aux requêtes en lecture seule alors que le serveur maître
    est en cours d'exécution. Le serveur esclave est idéal pour les requêtes
    vers un entrepôt de données.
   </para>

   <para>
    <productname>Slony-I</productname> est un exemple de ce type de
    réplication, avec une granularité par
    table et un support des esclaves multiples. Comme il met à jour le serveur
    esclave de façon asynchrone (par lots), il existe une possibilité de perte
    de données pendant un <foreignphrase>failover</foreignphrase>.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term><foreignphrase>Middleware</foreignphrase> de réplication basé sur les
    instructions</term>
  <listitem>

   <para>
    Avec les <foreignphrase>middleware</foreignphrase> de réplication basés
    sur les instructions, un programme intercepte chaque requête SQL et
    l'envoie à un ou tous les serveurs. Chaque serveur opère indépendamment.
    Les requêtes en lecture/écriture sont envoyées à tous les serveurs alors
    que les requêtes en lecture seule ne peuvent être envoyées qu'à un seul
    serveur, ce qui permet de distribuer la charge de lecture.
   </para>

   <para>
    Si les requêtes sont envoyées sans modification, les fonctions comme
    <function>random()</function>, <function>CURRENT_TIMESTAMP</function> ainsi
    que les séquences ont des valeurs différentes sur les différents serveurs.
    Cela parce que chaque serveur opère indépendamment alors que
    les requêtes SQL sont diffusées (et non les données
    modifiées). Si cette solution est inacceptable, le
    <foreignphrase>middleware</foreignphrase> ou l'application doivent
    demander ces valeurs à un seul serveur, et les utiliser dans
    des requêtes d'écriture. Une autre solution est d'utiliser cette solution de réplication
    avec une configuration maître-esclave traditionnelle, c'est à dire que les requêtes
    de modification de données ne sont envoyées qu'au maître et sont propagées aux
    esclaves via une réplication maître-esclave, pas par le middleware de
    réplication.  Il est impératif que
    toute transaction soit validée ou annulée sur tous les serveurs,
    éventuellement par validation en deux phases (<xref
    linkend="sql-prepare-transaction"/> et <xref linkend="sql-commit-prepared"/>.
    <productname>Pgpool-II</productname> et <productname>Sequoia</productname>
    sont des exemples de ce type de réplication.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Réplication asynchrone multi-maîtres</term>
  <listitem>

   <para>
    Pour les serveurs qui ne sont pas connectés en permanence, comme les
    ordinateurs portables ou les serveurs distants, conserver la cohérence des données
    entre les serveurs est un challenge. L'utilisation de la réplication asynchrone
    multi-maîtres permet à chaque serveur de fonctionner indépendamment. Il
    communique alors périodiquement avec les autres serveurs pour identifier les transactions
    conflictuelles. La gestion des conflits est alors confiée aux utilisateurs
    ou à un système de règles de résolution.
		Bucardo est un exemple de ce type de réplication.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Réplication synchrone multi-maîtres</term>
  <listitem>

   <para>
    Dans les réplications synchrones multi-maîtres, tous les serveurs acceptent
    les requêtes en écriture. Les données modifiées sont transmises
    du serveur d'origine à tous les autres serveurs avant toute validation de
    transaction.
   </para>
   <para>
    Une activité importante en écriture peut être la cause d'un
    verrouillage excessif et conduire à un effondrement des performances. Dans
    les faits, les performances en écriture sont souvent pis que celles d'un
    simple serveur.
   </para>
   <para>
    Tous les serveurs acceptent les requêtes en lecture.
   </para>
   <para>
    Certaines implantations utilisent les disques partagés pour réduire la surcharge
    de communication.
   </para>
   <para>
    Les performances de la réplication synchrone multi-maîtres sont meilleures lorsque
    les opérations de lecture représentent l'essentiel de la charge, alors que
    son gros avantage est l'acceptation des requêtes d'écriture par tous les
    serveurs &mdash; 
    il n'est pas nécessaire de répartir la charge entre les serveurs
    maîtres et esclaves et, parce que les modifications de données sont envoyées
    d'un serveur à l'autre, les fonctions non déterministes, comme
    <function>random()</function>, ne posent aucun problème.
   </para>

   <para>
    <productname>PostgreSQL</productname> n'offre pas ce type de réplication,
    mais la validation en deux phases de <productname>PostgreSQL</productname>
    (<xref linkend="sql-prepare-transaction"/> et <xref linkend="sql-commit-prepared"/>)
    autorise son intégration dans une application ou un
    <foreignphrase>middleware</foreignphrase>.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Solutions commerciales</term>
  <listitem>

   <para>
    Parce que <productname>PostgreSQL</productname> est libre et facilement
    extensible, certaines sociétés utilisent <productname>PostgreSQL</productname>
    dans des solutions commerciales fermées
    (<foreignphrase>closed-source</foreignphrase>) proposant des fonctionnalités de
    bascule sur incident (<foreignphrase>failover</foreignphrase>),
    réplication et répartition de charge.
   </para>
  </listitem>
 </varlistentry>

 </variablelist>

 <para>
  La <xref linkend="high-availability-matrix"/> résume les
  possibilités des différentes solutions listées plus-haut.
 </para>

 <table id="high-availability-matrix">
  <title>Matrice de fonctionnalités&nbsp;: haute disponibilité, répartition de
    charge et réplication</title>
  <tgroup cols="8">
   <thead>
    <row>
     <entry>Fonctionnalité</entry>
     <entry>Bascule par disques partagés (<foreignphrase>Shared Disk
     Failover</foreignphrase>)</entry>
     <entry>Réplication par système de fichiers</entry>
     <entry>Secours semi-automatique (<foreignphrase>Hot/Warm
     Standby</foreignphrase>) par <acronym>PITR</acronym></entry>
     <entry>Réplication maître/esclave basé sur les triggers</entry>
     <entry><foreignphrase>Middleware</foreignphrase> de réplication
       sur instructions</entry>
     <entry>Réplication asynchrone multi-maîtres</entry>
     <entry>Réplication synchrone multi-maîtres</entry>
    </row>
   </thead>

   <tbody>

    <row>
     <entry>Exemple d'implémentation</entry>
     <entry align="center">NAS</entry>
     <entry align="center">DRBD</entry>
     <entry align="center">PITR</entry>
     <entry align="center">Slony</entry>
     <entry align="center">pgpool-II</entry>
     <entry align="center">Bucardo</entry>
     <entry align="center"></entry>
    </row>

    <row>
     <entry>Méthode de communication</entry>
     <entry align="center">Disque partagé</entry>
     <entry align="center">Blocs disque</entry>
     <entry align="center">WAL</entry>
     <entry align="center">Lignes de tables</entry>
     <entry align="center">SQL</entry>
     <entry align="center">Lignes de tables</entry>
     <entry align="center">Lignes de tables et verrous de ligne</entry>
    </row>

    <row>
     <entry>Ne requiert aucun matériel spécial </entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Autorise plusieurs serveurs maîtres </entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Pas de surcharge sur le serveur maître </entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
    </row>

    <row>
     <entry>Pas d'attente entre serveurs</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
    </row>

    <row>
     <entry>Pas de perte de données en cas de panne du maître</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Les esclaves acceptent les requêtes en lecture seule</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">Hot only</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Granularité de niveau table</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Ne nécessite pas de résolution de conflit</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
    </row>

   </tbody>
  </tgroup>
 </table>

 <para>
  Certaines solutions n'entrent pas dans les catégories ci-dessus&nbsp;:
 </para>

 <variablelist>

 <varlistentry>
  <term>Partitionnement de données</term>
  <listitem>

   <para>
    Le partitionnement des données divise les tables en ensembles de données.
    Chaque ensemble ne peut être modifié que par un seul serveur. Les
    données peuvent ainsi être partitionnées par bureau, Londres et
    Paris, par exemple, avec un serveur dans chaque bureau. Si certaines
    requêtes doivent combiner des données de Londres et Paris, il est possible
    d'utiliser une application qui requête les deux serveurs ou d'implanter une
    réplication maître/esclave pour conserver sur chaque serveur une copie en lecture
    seule des données de l'autre bureau.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Exécution de requêtes en parallèle sur plusieurs serveurs</term>
  <listitem>

   <para>
    La plupart des solutions ci-dessus permettent à plusieurs serveurs de
    répondre à des requêtes multiples, mais aucune ne permet à une seule requête
    d'être exécutée sur plusieurs serveurs pour se terminer plus rapidement.
    Cette solution autorisent plusieurs serveurs à travailler ensemble sur une
    seule requête. Ceci s'accomplit habituellement en répartissant les données
    entre les serveurs, chaque serveur exécutant une partie de la
    requête pour renvoyer les résultats à un serveur central qui les combine
    et les renvoie à l'utilisateur. <productname>Pgpool-II</productname>
    offre cette possibilité. Cela peut également être implanté en utilisant les
    outils <productname>PL/Proxy</productname>.
   </para>
  </listitem>
 </varlistentry>

 </variablelist>

 </sect1>

 <sect1 id="warm-standby">
 <title>Serveurs de Standby par transfert de journaux</title>


  <para>
   L'archivage en continu peut être utilisé pour créer une configuration
   de cluster en <firstterm>haute disponibilité</firstterm> (HA) avec un ou
   plusieurs <firstterm>serveurs de standby</firstterm> prêts à prendre la main
   sur les opérations si le serveur primaire fait défaut. Cette fonctionnalité
   est généralement appelée 
   <firstterm>warm standby</firstterm> ou <firstterm>log shipping</firstterm>.
  </para>

  <para>
   Les serveurs primaire et de standby travaillent de concert pour fournir cette fonctionnalité,
   bien que les serveurs ne soient que faiblement couplés. Le serveur primaire opère
   en mode d'archivage en continu, tandis que le serveur de standby opère en
   mode de récupération en continu, en lisant les fichiers WAL provenant du primaire. Aucune
   modification des tables de la base ne sont requises pour activer cette fonctionnalité,
   elle entraîne donc moins de travail d'administration par rapport à d'autres
   solutions de réplication. Cette configuration a aussi un impact relativement
   faible sur les performances du serveur primaire.
  </para>

  <para>
   Déplacer directement des enregistrements de WAL d'un serveur de bases de données à un autre
   est habituellement appelé log shipping. <productname>PostgreSQL</productname>
   implémente le log shipping par fichier, ce qui signifie que les enregistrements de WAL sont
   transférés un fichier (segment de WAL) à la fois. Les fichiers de WAL (16Mo) peuvent être
   transférés facilement et de façon peu coûteuse sur n'importe quelle distance, que ce soit sur un
   système adjacent, un autre système sur le même site, ou un autre système à
   l'autre bout du globe. La bande passante requise pour cette technique
   varie en fonction du débit de transactions du serveur primaire.
   Le log shipping par enregistrement est aussi possible avec la streaming replication
   (voir <xref linkend="streaming-replication"/>).
  </para>

  <para>
   Il convient de noter que le log shipping est asynchrone, c'est à dire que les 
   enregistrements de WAL sont transférés après que la transaction ait été validée. Par conséquent, il y a
   un laps de temps pendant lequel une perte de données pourrait se produire si le serveur primaire
   subissait un incident majeur; les transactions pas encore transférées seront perdues. La taille de la fenêtre
   de temps de perte de données peut être réduite par l'utilisation du paramètre
   <varname>archive_timeout</varname>, qui peut être abaissé à des valeurs
   de quelques secondes. Toutefois, un paramètre si bas augmentera de façon 
   considérable la bande passante nécessaire pour le transfert de fichiers.
   Si vous voulez une fenêtre de moins d'une minute environ, envisagez l'utilisation
   de la streaming replication (voir <xref linkend="streaming-replication"/>).
  </para>

  <para>
   La performance de la récupération est suffisamment bonne pour que le standby ne
   soit en général qu'à quelques instants de la pleine
   disponibilité à partir du moment où il aura été activé. C'est pour cette raison que
   cette configuration de haute disponibilité est appelée warm standby.
   Restaurer un serveur d'une base de sauvegarde archivée, puis appliquer tous les journaux
   prendra largement plus de temps, ce qui fait que cette technique est une solution
   de 'disaster recovery' (reprise après sinistre), pas de haute disponibilité.
   Un serveur de standby peut aussi être utilisé pour des requêtes en lecture seule, dans
   quel cas il est appelé un serveur de Hot Standby. Voir <xref linkend="hot-standby"/> pour
   plus d'information.
 </para>

  <indexterm zone="high-availability">
   <primary>warm standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>PITR standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>serveur de standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>log shipping</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>serveur témoin</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>STONITH</primary>
  </indexterm>

  <sect2 id="standby-planning">
   <title>Préparatifs</title>

   <para>
    Il est habituellement préférable de créer les serveurs primaire et de standby
    de façon à ce qu'ils soient aussi similaires que possible, au moins du
    point de vue du serveur de bases de données. En particulier, les chemins
    associés avec les tablespaces seront passés d'un noeud à l'autre sans conversion, ce qui
    implique que les serveurs primaire et de standby doivent avoir les mêmes chemins de montage pour
    les tablespaces si cette fonctionnalité est utilisée. Gardez en tête que si
    <xref linkend="sql-createtablespace"/> 
    est exécuté sur le primaire, tout nouveau point de montage nécessaire pour cela doit être créé
    sur le primaire et tous les standby avant que la commande ne
    soit exécutée. Le matériel n'a pas besoin d'être exactement le même, mais l'expérience monte
    que maintenir deux systèmes identiques est plus facile que maintenir deux
    différents sur la durée de l'application et du système.
    Quoi qu'il en soit, l'architecture hardware doit être la même &mdash; répliquer
    par exemple d'un serveur 32 bits vers un 64 bits ne fonctionnera pas.
   </para>

   <para>
    De manière générale, le log shipping entre serveurs exécutant des versions 
    majeures différentes de <productname>PostgreSQL</productname> est
    impossible. La politique du PostgreSQL Global Development Group est de ne pas
    réaliser de changement sur les formats disques lors des mises à jour mineures,
    il est par conséquent probable que l'exécution de versions mineures différentes
    sur le primaire et le standby fonctionne correctement. Toutefois, il n'y a 
    aucune garantie formelle de cela et il est fortement conseillé de garder le 
    serveur primaire et celui de standby au même niveau de version autant que faire
    se peut. Lors d'une mise à jour vers une nouvelle version mineure, la politique la
    plus sûre est de mettre à jour les serveurs de standby d'abord &mdash; une nouvelle
    version mineure est davantage susceptible de lire les enregistrements WAL d'une
    ancienne version mineure que l'inverse.
   </para>

  </sect2>

  <sect2 id="standby-server-operation">
   <title>Fonctionnement du Serveur de Standby</title>

   <para>
    En mode de standby, le serveur applique continuellement les WAL reçus du
    serveur maître. Le serveur de standby peut lire les WAL d'une archive WAL
    (voir <varname>restore_command</varname>) ou directement du maître via une
    connexion TCP (streaming replication). Le serveur de standby essaiera aussi de
    restaurer tout WAL trouvé dans le répertoire <filename>pg_xlog</filename> du
    cluster de standby. Cela se produit habituellement après un redémarrage de
    serveur, quand le standby rejoue à nouveau les WAL qui ont été reçu du maître
    avant le redémarrage, mais vous pouvez aussi copier manuellement des fichiers dans
    <filename>pg_xlog</filename> à tout moment pour qu'ils soient rejoués.
    /para>

   <para>
    Au démarrage, le serveur de standby commence par restaurer tous les WAL
    disponibles à l'endroit où se trouvent les archives, en appelant la
    <varname>restore_command</varname>. Une fois qu'il a épuisé tous les WAL
    disponibles à cet endroit et que <varname>restore_command</varname>
    échoue, il essaye de restaurer tous les WAL disponibles dans le répertoire
    pg_xlog. Si cela échoue, et que la streaming replication a été activée, le standby essaye
    de se connecter au serveur primaire et de démarrer la réception des WAL depuis
    le dernier enregistrement valide trouvé dans les archives ou pg_xlog. Si cela
    échoue ou que la streaming replication n'est pas configurée, ou que la connexion
    est plus tard déconnectée, le standby retourne à l'étape 1 et essaye de 
    restaurer le fichier à partir de l'archive à nouveau. Cette boucle de
    retentatives de l'archive, pg_xlog et par la streaming replication continue
    jusqu'à ce que le serveur soit stoppé ou que le failover (bascule) soit
    déclenché par un fichier trigger (déclencheur).
   </para>

   <para>
    Le mode de standby est quitté et le serveur bascule en mode de fonctionnement normal
    quand un fichier de trigger est trouvé (<varname>trigger_file</varname>). Avant
    de basculer, tout WAL immédiatement disponible dans l'archive ou le pg_xlog sera
    restaurée, mais aucune tentative ne sera faite pour se connecter au maître.
   </para>
  </sect2>

  <sect2 id="preparing-master-for-standby">
   <title>Préparer le Maître pour les Serveurs de Standby</title>

   <para>
    Mettez en place un archivage en continu sur le primaire vers un répertoire
    d'archivage accessible depuis le standby, comme décrit 
    dans <xref linkend="continuous-archiving"/>. La destination d'archivage devrait être
    accessible du standby même quand le maître est inaccessible, c'est à dire qu'il
    devrait se trouver sur le serveur de standby lui-même ou un autre serveur de confiance, pas sur
    le serveur maître.
   </para>

   <para>
    Si vous voulez utiliser la streaming replication, mettez en place l'authentification sur le
    serveur primaire pour autoriser les connexions de réplication à partir du (des) serveur de
    standby; c'est à dire, mettez en place une ou des entrées appropriées dans
    <filename>pg_hba.conf</filename> avec le champ database positionné à
    <literal>replication</literal>. Vérifiez aussi que <varname>max_wal_senders</varname> est positionné
    à une valeur suffisamment grande dans le fichier de configuration du serveur primaire.
   </para>

   <para>
    Effectuez une sauvegarde de base comme décrit dans <xref linkend="backup-base-backup"/>
    pour initialiser le serveur de standby.
   </para>
  </sect2>

  <sect2 id="standby-server-setup">
   <title>Paramétrer un Serveur de Standby</title>

   <para>
    Pour paramétrer le serveur de standby, restaurez la sauvegarde de base effectué sur
    le serveur primaire (voir (see <xref linkend="backup-pitr-recovery"/>). Créez un
    fichier de commande de récupération <filename>recovery.conf</filename> dans
    le répertoire de données du cluster de standby, et positionnez <varname>standby_mode</varname>
    à on. Positionnez <varname>restore_command</varname> à une simple commande qui recopie
    les fichiers de l'archive de WAL.
   </para>

   <note>
     <para>
     N'utilisez pas pg_standby ou des outils similaires avec le mode de standby intégré
     décrit ici. <varname>restore_command</varname> devrait retourner immédiatement
     si le fichier n'existe pas; le serveur essayera la commande à nouveau si nécessaire.
     Voir <xref linkend="log-shipping-alternative"/> pour utiliser des outils tels que pg_standby.
    </para>
   </note>

   <para>
     Si vous souhaitez utiliser la streaming replication, renseignez
     <varname>primary_conninfo</varname> avec une chaîne de connexion libpq,
     contenant le nom d'hôte (ou l'adresse IP) et tout détail supplémentaire
     nécessaire pour se connecter au serveur primaire. Si le primaire a besoin d'un
     mot de passe pour l'authentification, le mot de passe doit aussi être spécifié dans
     <varname>primary_conninfo</varname>.
   </para>

   <para>
    Vous pouvez utiliser <varname>restartpoint_command</varname> pour supprimer de
    l'archive les fichiers qui ne sont plus nécessaires à la base de standby.
   </para>

   <para>
    Si vous mettez en place le serveur de standby pour des besoins de haute disponibilité,
    mettez en place l'archivage de WAL, les connexions et l'authentification à l'identique
    du serveur primaire, parce que le serveur de standby fonctionnera comme un serveur primaire
    après la bascule. Vous aurez aussi besoin de positionner <varname>trigger_file</varname>
    pour rendre la bascule possible.
    Si vous mettez en place le serveur de standby pour des besoins de reporting, sans aucune
    intention de basculer dessus, <varname>trigger_file</varname> n'est pas nécessaire.
   </para>

   <para>
    Un simple exemple de <filename>recovery.conf</filename> est:
<programlisting>
standby_mode = 'on'
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
restore_command = 'cp /path/to/archive/%f %p'
trigger_file = '/path/to/trigger_file'
</programlisting>
   </para>

   <para>
    Vous pouvez avoir n'importe quel nombre de serveurs de standby, mais si vous
    utilisez la streaming replication, assurez vous d'avoir positionné
    <varname>max_wal_senders</varname> suffisamment haut sur le primaire pour leur permettre
    de se connecter simultanément.
   </para>

   <para>
    Si vous utilisez une archive WAL, sa taille peut être réduite en utilisant
    l'option <varname>restartpoint_command</varname> pour supprimer les fichiers
    qui ne sont plus nécessaires au serveur de standby. Notez toutefois que si vous
    utilisez l'archive à des fins de sauvegarde, vous avez besoin de garder les fichiers
    nécessaires pour restaurer à partir de votre dernière sauvegarde de base, même
    si ces fichiers ne sont plus nécessaires pour le standby.
   </para>
  </sect2>

  <sect2 id="streaming-replication">
   <title>Streaming Replication</title>

   <indexterm zone="high-availability">
    <primary>Streaming Replication</primary>
   </indexterm>

   <para>
    La streaming replication permet à un serveur de standby de rester plus
    à jour qu'il n'est possible avec l'envoi de journaux par fichiers. Le
    standby se connecte au primaire, qui envoie au standby les enregistrements
    de WAL dès qu'ils sont générés, sans attendre qu'un fichier de WAL soit rempli.
   </para>

   <para>
    La streaming replication est asynchrone, il y a donc toujours un petit délai
    entre la validation d'une transaction sur le primaire et le moment où les
    changements sont visibles sur le standby. Le délai est toutefois beaucoup plus petit
    qu'avec l'envoi de fichiers, habituellement en dessous d'une seconde en partant
    de l'hypothèse que le standby est suffisamment puissant pour supporter la charge. Avec
    la streaming replication, <varname>archive_timeout</varname> n'est pas nécessaire
    pour réduire la fenêtre de perte de données.
   </para>

   <para>
    Si vous utilisez la streaming replication sans archivage en continu des fichiers,
    vous devez positionner <varname>wal_keep_segments</varname> sur le maître à une valeur
    suffisamment grande pour garantir que les anciens segments de WAL ne sont pas
    recyclés trop tôt, alors que le standby pourrait toujours avoir besoin d'eux pour
    rattraper son retard. Si le standby prend trop de retard, il aura besoin d'être réinitialisé
    à partir d'une nouvelle sauvegarde de base. Si vous positionnez une archive de WAL qui est accessible
    du standby, wal_keep_segments n'est pas nécessaire, puisque le standby peut toujours
    utiliser l'archive pour rattraper son retard.
   </para>

   <para>
    Pour utiliser la streaming replication, mettez en place un serveur de standby
    en mode fichier comme décrit dans <xref linkend="warm-standby"/>. L'étape qui
    transforme un standby en mode fichier en standby en streaming replication est de
    faire pointer <varname>primary_conninfo</varname> dans le fichier
    <filename>recovery.conf</filename> vers le serveur primaire. Positionnez
    <xref linkend="guc-listen-addresses"/> et les options d'authentification
    (voir <filename>pg_hba.conf</filename>) sur le primaire pour que le serveur
    de standby puisse se connecter à la pseudo-base <literal>replication</literal> 
    sur le serveur primaire (voir <xref linkend="streaming-replication-authentication"/>).
   </para>

   <para>
    Sur les systèmes qui supportent l'option de keepalive sur les sockets, positionner
    <xref linkend="guc-tcp-keepalives-idle"/>,
    <xref linkend="guc-tcp-keepalives-interval"/> et
    <xref linkend="guc-tcp-keepalives-count"/> aide le primaire à reconnaître rapidement
    une connexion interrompue.
   </para>

   <para>
    Positionnez le nombre maximum de connexions concurrentes à partir des
    serveurs de standby (voir <xref linkend="guc-max-wal-senders"/> pour les détails).
   </para>

   <para>
    Quand le standby est démarré et que <varname>primary_conninfo</varname> est
    positionné correctement, le standby se connectera au primaire après avoir
    rejoué tous les fichiers WAL disponibles dans l'archive. Si la connexion
    est établie avec succès, vous verrez un processus walreceiver dans le standby, et
    un processus walsender correspondant sur le primaire.
   </para>

   <sect3 id="streaming-replication-authentication">
    <title>Authentification</title>
    <para>
     Il est très important que les privilèges d'accès pour la réplications soient paramétrés
     pour que seuls les utilisateurs de confiance puissent lire le flux WAL, parce qu'il
     est facile d'en extraire des informations privilégiées. Les serveurs de standby
     doivent s'authentifier sur le primaire avec un compte superutilisateur.
     Par conséquent, un rôle avec les privilèges
     <literal>SUPERUSER</literal> et <literal>LOGIN</literal> doit être créé sur le primaire.
    </para>
    <para>
     L'authentification cliente pour la réplication est contrôlée par un enregistrement de
     <filename>pg_hba.conf</filename> spécifiant <literal>replication</literal> dans le champ
     <replaceable>database</replaceable>. Par exemple, si le standby s'exécute sur un hôte d'IP
     <literal>192.168.1.100</literal>  et que le nom du superutilisateur pour la réplication est
     <literal>foo</literal>, l'administrateur peut ajouter la ligne suivante au fichier
     <filename>pg_hba.conf</filename>  sur le primaire:
     

<programlisting>
# Autoriser l'utilisateur "foo" de l'hôte 192.168.1.100 à se connecter au primaire
# en tant que standby de replication si le mot de passe de l'utilisateur est correctement fourni
#
# TYPE  DATABASE        USER            CIDR-ADDRESS            METHOD
host    replication     foo             192.168.1.100/32        md5
</programlisting>
    </para>
    <para>
     Le nom d'hôte et le numéro de port du primaire, le nom d'utilisateur de la connexion,
     et le mot de passe sont spécifiés dans le fichier <filename>recovery.conf</filename>
     ou dans les variables d'environnement correspondantes sur le standby.
     Par exemple, si le primaire s'exécute sur l'hôte d'IP <literal>192.168.1.50</literal>,
     port <literal>5432</literal>, que le nom du superutilisateur pour la réplication est
     <literal>foo</literal>, et que le mot de passe est <literal>foopass</literal>, l'administrateur
     peut ajouter la ligne suivante au fichier <filename>recovery.conf</filename> sur le standby:

<programlisting>
# Le standby se connecte au primaire qui s'exécute sur l'hôte 192.168.1.50
# et port 5432 en tant qu'utilisateur "foo" dont le mot de passe est "foopass"
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
</programlisting>
    </para>
   </sect3>

   <sect3 id="streaming-replication-monitoring">
    <title>Supervision</title>
    <para>
     Un important indicateur de santé de la streaming replication est le nombre
     d'enregistrements générés sur le primaire, mais pas encore appliqués sur
     le standby. Vous pouvez calculer ce retard en comparant le point d'avancement
     des écritures du WAL sur le primaire avec le dernier point d'avancement reçu par
     le standby. Ils peuvent être récupérés en utilisant
     <function>pg_current_xlog_location</function> sur le primaire et
     <function>pg_last_xlog_receive_location</function> sur le standby,
     respectivement (voir <xref linkend="functions-admin-backup-table"/> et
     <xref linkend="functions-recovery-info-table"/> pour plus de détails).
     Le point d'avancement de la réception dans le standby est aussi affiché dans
     le statut du processus de réception des WAL (wal receiver), affiché par
     la commande <command>ps</command> (voyez <xref linkend="monitoring-ps"/> pour plus de détails).
    </para>
   </sect3>

  </sect2>
  </sect1>

  <sect1 id="warm-standby-failover">
   <title>Failover (bascule)</title>

   <para>
    Si le serveur primaire plante alors le serveur de standby devrait commencer
    les procédures de failover.
   </para>

   <para>
    Si le serveur de standby plante alors il n'est pas nécessaire d'effectuer un failover. Si le
    serveur de standby peut être redémarré, même plus tard, alors le processus de récupération
    peut aussi être redémarré au même moment, en bénéficiant du fait que la récupération sait reprendre
    où elle en était. Si le serveur de standby ne peut pas être redémarré, alors
    une nouvelle instance complète de standby devrait être créé.
   </para>

   <para>
    Si le serveur primaire plante, que le serveur de standby devient le 
    nouveau primaire, et que l'ancien primaire redémarre, vous devez avoir
    un mécanisme pour informer l'ancien primaire qu'il n'est plus primaire. C'est aussi
    quelquefois appelé <acronym>STONITH</acronym> (Shoot The Other Node In The Head, ou
    Tire Dans La Tête De L'Autre Noeud), qui est nécessaire pour éviter les situations où
    les deux systèmes pensent qu'ils sont le primaire, ce qui amènerait de la confusion, et
    finalement de la perte de données.
   </para>

   <para>
    Beaucoup de systèmes de failover n'utilisent que deux systèmes, le primaire et le standby,
    connectés par un mécanisme de type ligne de vie (heartbeat) pour vérifier continuellement la
    connexion entre les deux et la viabilité du primaire. Il est aussi
    possible d'utiliser un troisième système (appelé un serveur témoin) pour éviter
    certains cas de bascule inappropriés, mais la complexité supplémentaire
    peut ne pas être justifiée à moins d'être mise en place avec suffisamment
    de précautions et des tests rigoureux.
   </para>

   <para>
    <productname>PostgreSQL</productname> ne fournit pas le logiciel
    système nécessaire pour identifier un incident sur le primaire et notifier
    le serveur de base de standby. De nombreux outils de ce genre existent et sont bien
    intégrés avec les fonctionnalités du système d'exploitation nécessaires à la bascule,
    telles que la migration d'adresse IP.
   </para>

   <para>
    Une fois que la bascule vers le standby se produit, il n'y a plus qu'un
    seul serveur en fonctionnement. C'est ce qu'on appelle un état dégradé.
    L'ancien standby est maintenant le primaire, mais l'ancien primaire est arrêté
    et pourrait rester arrêté. Pour revenir à un fonctionnement normal, un serveur
    de standby doit être recréé,
    soit sur l'ancien système primaire quand il redevient disponible, ou sur un troisième,
    peut être nouveau, système. Une fois que ceci est effectué, le primaire et le standby peuvent
    être considérés comme ayant changé de rôle. Certaines personnes choisissent d'utiliser un troisième
    serveur pour fournir une sauvegarde du nouveau primaire jusqu'à ce que le nouveau serveur de
    standby soit recréé,
    bien que ceci complique visiblement la configuration du système et les procédures d'exploitation.
   </para>

   <para>
    Par conséquent, basculer du primaire vers le serveur de standby peut être rapide mais requiert
    du temps pour re-préparer le cluster de failobver. Une bascule régulière du
    primaire vers le standby est utile, car cela permet une période d'interruption de production sur
    chaque système pour maintenance. Cela vous permet aussi pour vous assurer que 
    votre mécanisme de bascule fonctionnera réellement quand vous en aurez besoin.
		Il est conseillé que les procédures d'administration soient écrites.
   </para>

   <para>
    Pour déclencher le failover d'un serveur de standby en log-shipping, créez un fichier
    trigger (déclencheur) avec le nom de fichier et le chemin spécifiés par le paramètre
    <varname>trigger_file</varname> de <filename>recovery.conf</filename>. Si <varname>trigger_file</varname>
    n'est pas fourni, il n'y a aucun moyen de sortir du mode de recovery sur le standby et de le promouvoir
    en maître. Ceci peut être utile, par exemple, pour des serveurs de reporting qui ne sont
    utilisés que pour décharger le primaire de requêtes en lecture seule, pas à des fins de
    haute disponibilité.
   </para>
  </sect1>

  <sect1 id="log-shipping-alternative">
   <title>Méthode alternative pour le log shipping</title>

   <para>
    Une alternative au mode de standby intégré décrit dans les sections précédentes
    est d'utiliser une <varname>restore_command</varname> qui scrute le dépôt d'archives.
    C'était la seule méthode disponible dans les versions 8.4 et inférieures. Dans cette configuration,
    positionnez <varname>standby_mode</varname>  à off, parce que vous implémentez la scrutation nécessaire
    au fonctionnement standby vous-mêmes. Voyez contrib/pg_standby (<xref linkend="pgstandby"/>) pour
    une implémentation de référence de ceci.
   </para>

   <para>
    Veuillez noter que dans ce mode, le serveur appliquera les WAL fichier par fichier,
    ce qui entraîne que si vous requêtez sur le serveur de standby (voir Hot Standby),
    il y a un délai entre une action sur le maître et le moment où cette action
    devient visible sur le standby, correspondant au temps nécessaire à 
    remplir le fichier de WAL. <varname>archive_timeout</varname>  peut être utilisé pour rendre ce délai
    plus court. Notez aussi que vous ne pouvez combiner la streaming replication avec cette méthode.
   </para>

   <para>
    Les opérations qui se produisent sur le primaire et les serveurs de standby sont
    des opérations normales d'archivage et de recovery. Le seul point de
    contact entre les deux serveurs de bases de données est l'archive de fichiers WAL
    qu'ils partagent: le primaire écrivant dans l'archive, le secondaire
    lisant de l'archive. Des précautions doivent être prises pour s'assurer que les archives WAL de serveurs
    primaires différents ne soient pas mélangées ou confondues. L'archive n'a pas besoin
    d'être de grande taille si elle n'est utilisée que pour le fonctionnement de standby.
   </para>

   <para>
    La magie qui permet aux deux serveurs faiblement couplés de fonctionner ensemble est
    une simple <varname>restore_command</varname> utilisée sur le standby qui
    quand on lui demande le prochain fichier de WAL, attend que le primaire le mette
    à disposition. La <varname>restore_command</varname>  est spécifiée dans le
    fichier <filename>recovery.conf</filename>  sur le serveur de standby. La récupération normale
    demanderait un fichier de l'archive WAL, en retournant un échec si le
    fichier n'était pas disponible. Pour un fonctionnement en standby, il est normal que
    le prochain fichier WAL ne soit pas disponible, ce qui entraîne que le standby doive attendre
    qu'il apparaisse. Pour les fichiers se terminant en <literal>.backup</literal> ou
    <literal>.history</literal> il n'y a pas besoin d'attendre, et un code retour
    différent de zéro doit être retourné. Une <varname>restore_command</varname>  d'attente
    peut être écrite comme un script qui boucle après avoir scruté l'existence du prochain fichier de WAL.
    Il doit aussi y avoir un moyen de déclencher la bascule, qui devrait interrompre la
    <varname>restore_command</varname> , sortir le la boucle et retourner une erreur file-not-found 
    au serveur de standby. Cela met fin à la récupération et le standby démarrera alors comme un serveur normal.
   </para>

   <para>
    Le pseudocode pour une <varname>restore_command</varname> appropriée est:
<programlisting>
triggered = false;
while (!NextWALFileReady() &amp;&amp; !triggered)
{
    sleep(100000L);         /* wait for ~0.1 sec */
    if (CheckForExternalTrigger())
        triggered = true;
}
if (!triggered)
        CopyWALFileForRecovery();
</programlisting>
   </para>

   <para>
    Un exemple fonctionnel de <varname>restore_command</varname> d'attente est fournie
    par le module <filename>contrib</filename>  appelé <application>pg_standby</application>. Il
    devrait être utilisé en tant que référence, comme la bonne façon d'implémenter correctement la logique
    décrite ci-dessus. Il peut aussi être étendu pour supporter des configurations et des 
    environnements spécifiques.
   </para>

   <para>
    La méthode pour déclencher une bascule est une composante importante de la
    planification et de la conception. Une possibilité est d'utiliser la
    commande <varname>restore_command</varname>. Elle est exécutée une fois
    pour chaque fichier WAL, mais le processus exécutant la <varname>restore_command</varname>
    est créé et meurt pour chaque fichier, il n'y a donc ni démon ni processus serveur, et
    on ne peut utiliser ni signaux ni gestionnaire de signaux.  Par conséquent, la
    <varname>restore_command</varname> n'est pas appropriée pour déclencher la bascule.
    Il est possible d'utiliser une simple fonctionnalité de timeout, particulièrement
    si utilisée en conjonction avec un paramètre <varname>archive_timeout</varname>
    sur le primaire. Toutefois, ceci est sujet à erreur, un problème réseau
    ou un serveur primaire chargé pouvant suffire à déclencher une bascule. Un système
    de notification comme la création explicite d'un fichier trigger est idéale, dans la
    mesure du possible.
   </para>

  <sect2 id="warm-standby-config">
   <title>Implémentation</title>

   <para>
    La procédure simplifié pour configurer un serveur de test en utilisant cette
    méthode alternative est la suivante. Pour tous les détails
    sur chaque étape, référez vous aux sections précédentes suivant les indications.
    <orderedlist>
     <listitem>
      <para>
       Paramétrez les systèmes primaire et standby de façon aussi identique que possible,
       y compris deux copies identiques de <productname>PostgreSQL</productname> au même niveau
       de version.
      </para>
     </listitem>
     <listitem>
      <para>
       Activez l'archivage en continu du primaire vers un répertoire d'archives WAL
       sur le serveur de standby. Assurez vous que 
       <xref linkend="guc-archive-mode"/>,
       <xref linkend="guc-archive-command"/> et
       <xref linkend="guc-archive-timeout"/>
       sont positionnés correctement sur le primaire
       (voir <xref linkend="backup-archiving-wal"/>).
      </para>
     </listitem>
     <listitem>
      <para>
       Effectuez une sauvegarde de base du serveur primaire( voir <xref
       linkend="backup-base-backup"/>), , et chargez ces données sur le standby.
      </para>
     </listitem>
     <listitem>
      <para>
       Commencez la récupération sur le serveur de standby à partir de l'archive WAL locale,
       en utilisant un <filename>recovery.conf</filename>  qui spécifie une
       <varname>restore_command</varname>  qui attend comme décrit
       précédemment (voir <xref linkend="backup-pitr-recovery"/>).
      </para>
     </listitem>
    </orderedlist>
   </para>

   <para>
    Le récupération considère l'archive WAL comme étant en lecture seule, donc une fois qu'un fichier WAL
    a été copié sur le système de standby il peut être copié sur bande en même temps
    qu'il est lu par le serveur de bases de données de standby.
    Ainsi, on peut faire fonctionner un serveur de standby pour de la haute disponibilité
    en même temps que les fichiers sont stockés pour de la reprise après sinistre.
   </para>

   <para>
    À des fins de test, il est possible de faire fonctionner le serveur primaire et
    de standby sur le même système. Cela n'apporte rien en termes de robustesse du serveur,
    pas plus que cela ne pourrait être décrit comme de la haute disponibilité.
   </para>
  </sect2>
.
  <sect2 id="warm-standby-record">
   <title>Log Shipping par Enregistrements</title>

   <para>
    Il est aussi possible d'implémenter du log shipping par enregistrements en utilisant
    cette méthode alternative, bien qu'elle nécessite des développements spécifiques,
    et que les modifications ne seront toujours visibles aux requêtes de hot standby qu'après
    que le fichier complet de WAL ait été recopié.
   </para>

   <para>
    Un programme externe peut appeler la fonction <function>pg_xlogfile_name_offset()</function>
    (voir <xref linkend="functions-admin"/>) pour obtenir le nom de fichier et la position exacte
    en octets dans ce fichier de la fin actuelle du WAL. Il peut alors accéder au fichier WAL directement
    et copier les données de la fin précédente connue à la fin courante vers les serveurs de standby.
    Avec cette approche, la fenêtre de perte de données est la période de scrutation du programme de copie,
    qui peut être très petite, et il n'y a pas de bande passante gaspillée en forçant l'archivage 
    de fichiers WAL partiellement remplis. Notez que les scripts <varname>restore_command</varname>
    des serveurs de standby ne peuvent traiter que des fichiers WAL complets, les données copiées
    de façon incrémentale ne sont donc d'ordinaire pas mises à  disposition des serveurs de standby.
    Elles ne sont utiles que si le serveur primaire tombe &mdash; alors le dernier fichier WAL partiel
    est fourni au standby avant de l'autoriser à s'activer. L'implémentation correcte de ce
    mécanisme requiert la coopération entre le script <varname>restore_command</varname> et
    le programme de recopie des données.
   </para>

   <para>
    À partir de <productname>PostgreSQL</productname> version 9.0, vous pouvez utiliser
    la streaming replication (voir <xref linkend="streaming-replication"/>) pour
    bénéficier des mêmes fonctionnalités avec moins d'efforts.
   </para>
  </sect2>
 </sect1>

 <sect1 id="hot-standby">
  <title>Hot Standby</title>

  <indexterm zone="high-availability">
   <primary>Hot Standby</primary>
  </indexterm>

   <para>
    Hot Standby est le terme utilisé pour décrire la possibilité de se
    connecter et d'exécuter des requêtes en lecture seule alors que le
    serveur est en récupération d'archive. C'est
    utile à la fois pour la réplication en log shipping et pour restaurer
    une sauvegarde à un état exact avec une grande précision.
    Le terme Hot Standby fait aussi référence à la capacité du serveur à passer
    de la récupération au fonctionnement normal tandis-que les utilisateurs
    continuent à exécuter des requêtes et/ou gardent leurs connexions ouvertes.
   </para>

   <para>
    Exécuter des requêtes en mode de récupération est similaire au fonctionnement
    normal des requêtes, bien qu'il y ait quelques différences d'utilisation
    et d'administration notées ci-dessous.
   </para>

  <sect2 id="hot-standby-users">
   <title>Aperçu pour l'utilisateur</title>

   <para>
    Les utilisateurs peuvent se connecter au serveur de base de données
    alors qu'il est en mode de récupération et exécuter des requêtes en
    lecture seule. L'accès en lecture seule aux catalogues et vues se
    produiront aussi de façon normale.
   </para>

   <para>
    Les données sur le standby mettent un certain temps pour arriver du serveur
    primaire, il y aura donc un délai mesurable entre primaire et standby. La même
    requête exécutée presque simultanément sur le primaire et le standby pourrait par
    conséquent retourner des résultats différents. On dit que la donnée est
    <firstterm>cohérente à terme</firstterm>  avec le primaire.
    Les requêtes exécutées sur le standby seront correctes du point de vue de la transaction
    qui aura été restaurée au démarrage de la requête, ou du démarrage de la première
    requête dans le cas de transactions sérialisables. Rapporté au primaire,
    le standby retourne des résultats de requêtes qui auraient pu être obtenus sur
    le primaire à un moment dans le passé.
   </para>

   <para>
    Quand une transaction est démarrée en récupération, le paramètre 
    <varname>transaction_read_only</varname> sera forcé à true, quelle que soit la valeur
    du paramètre <varname>default_transaction_read_only</varname> dans <filename>postgresql.conf</filename>.
    Il ne peut pas non plus être passé manuellement à false. Par conséquent, toutes les transactions
    démarrées durant la récupération seront limitées à des opérations en lecture seule. De tous les autres
    points de vue, les sessions connectées apparaîtront comme identiques à des sessions initiées durant
    le fonctionnement normal. Il n'y a pas besoin de commandes spéciales pour initier une connexion, les
    interfaces fonctionnent donc sans modification. Après que la récupération soit finie, la session autorisera
    les transactions en lecture-écriture normales au démarrage de la transaction suivante, si celles-ci sont
    demandées.
   </para>

   <para>
    Le "En lecture seule" ci-dessus signifie aucune écriture dans des tables
    permanentes ou temporaires. Il n'y a aucun problème avec les requêtes qui
    utilisent des fichiers temporaires de tri ou de travail.
   </para>

   <para>
    Les actions suivantes sont autorisées:
    <itemizedlist>
     <listitem>
      <para>
       Accès par requête - <command>SELECT</command>, <command>COPY TO</command> incluant les vues et les rules (règles)
       <command>SELECT</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de curseur - <command>DECLARE</command>, <command>FETCH</command>, <command>CLOSE</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Paramètres - <command>SHOW</command>, <command>SET</command>, <command>RESET</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de gestion de transaction
        <itemizedlist>
         <listitem>
          <para>
           <command>BEGIN</command>, <command>END</command>, <command>ABORT</command>, <command>START TRANSACTION</command>
          </para>
         </listitem>
         <listitem>
          <para>
           <command>SAVEPOINT</command>, <command>RELEASE</command>, <command>ROLLBACK TO SAVEPOINT</command>
          </para>
         </listitem>
         <listitem>
          <para>
           Blocs d'<command>EXCEPTION</command> et autres sous-transactions internes
          </para>
         </listitem>
        </itemizedlist>
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK TABLE</command>, mais seulement quand explicitement dans un de ces modes:
       <literal>ACCESS SHARE</literal>, <literal>ROW SHARE</literal> ou <literal>ROW EXCLUSIVE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Plans et ressources - <command>PREPARE</command>, <command>EXECUTE</command>,
       <command>DEALLOCATE</command>, <command>DISCARD</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Plugins et extensions - <command>LOAD</command>
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Ces actions produisent des messages d'erreur:

    <itemizedlist>
     <listitem>
      <para>
       Langage de Manipulation de Données (LMD ou DML) - <command>INSERT</command>,
       <command>UPDATE</command>, <command>DELETE</command>, <command>COPY FROM</command>,
       <command>TRUNCATE</command>.
       Notez qu'il n'y a pas d'action autorisée qui entraînerait l'exécution d'un 
       trigger pendant la récupération.
      </para>
     </listitem>
     <listitem>
      <para>
       Langage de Définition de Données (LDD ou DDL) - <command>CREATE</command>,
       <command>DROP</command>, <command>ALTER</command>, <command>COMMENT</command>.
       Cela s'applique aussi aux tables temporaires parce qu'à l'heure actuelle
       leur définition déclenche des écritures dans les tables du catalogue.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>SELECT ... FOR SHARE | UPDATE</command> qui entraînent l'écriture de verrous de ligne
      </para>
     </listitem>
     <listitem>
      <para>
       Rules sur des ordres <command>SELECT</command> qui génèrent des commandes LMD.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK</command> qui demandent explicitement un mode supérieur à <literal>ROW EXCLUSIVE MODE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK</command> dans sa forme courte par défaut, puisqu'il demande <literal>ACCESS EXCLUSIVE MODE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
      Commandes de gestion de transaction qui positionnent explicitement un état n'étant pas en lecture-seule:
        <itemizedlist>
         <listitem>
          <para>
            <command>BEGIN READ WRITE</command>,
            <command>START TRANSACTION READ WRITE</command>
          </para>
         </listitem>
         <listitem>
          <para>
            <command>SET TRANSACTION READ WRITE</command>,
            <command>SET SESSION CHARACTERISTICS AS TRANSACTION READ WRITE</command>
          </para>
         </listitem>
         <listitem>
          <para>
           <command>SET transaction_read_only = off</command>
          </para>
         </listitem>
        </itemizedlist>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de two-phase commit <command>PREPARE TRANSACTION</command>,
       <command>COMMIT PREPARED</command>, <command>ROLLBACK PREPARED</command>
       parce que même les transactions en lecture seule ont besoin d'écrire dans le WAL
       durant la phase de préparation (la première des deux phases du two-phase commit).
      </para>
     </listitem>
     <listitem>
      <para>
       Mise à jour de séquence - <function>nextval()</function>, <function>setval()</function>
      </para>
     </listitem>
     <listitem>
      <para>
       LISTEN, UNLISTEN, NOTIFY
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Notez que le comportement actuel des transactions en lecture seule hors récupération
    est d'autoriser les deux dernières actions, il y a donc de petites et subtiles
    différences de comportement entre des transactions en lecture seule
    lancées sur un standby et durant un fonctionnement normal.
    Il est possible que <command>LISTEN</command>, <command>UNLISTEN</command>,
    et les tables temporaires soient autorisées dans une version ultérieure.
   </para>

   <para>
    Si la bascule (failover ou switchover) est déclenchée la base basculera dans
    le mode de fonctionnement normal. Les sessions resteront connectées tandis que
    le serveur change de mode. Les transactions en cours continueront, mais resteront
    en lecture seule. Après que la récupération soit achevée, il sera possible de démarrer
    des transactions en lecture-écriture.
   </para>

   <para>
    Les utilisateurs pourront déterminer si leur session est en lecture seule en
    exécutant <command>SHOW transaction_read_only</command>. De plus, un jeu de
    fonctions (<xref linkend="functions-recovery-info-table"/>) permettent aux utilisateurs d'
    accéder à des informations à propos du serveur de standby. Ceci vous permet d'écrire
    des programmes qui sont conscients de l'état actuel de la base. Vous pouvez
    vous en servir pour superviser l'avancement de la récupération, ou pour écrire des
    programmes complexes qui restaurent la base dans des états particuliers.
   </para>

   <para>
    Durant la récupération, les transactions n'auront pas le droit de prendre de verrou de table
    plus élevés que <literal>RowExclusiveLock</literal>. Par ailleurs, les transactions n'ont
    pas le droit d'assigner un TransactionId et ne peuvent écrire de WAL.
    Toute commande <command>LOCK TABLE</command>  qui s'exécute sur le standby et requiert
    un mode de verrouillage supérieur à <literal>ROW EXCLUSIVE MODE</literal> sera rejetée.
   </para>

   <para>
    En général les requêtes ne subiront pas de conflits de verrouillage provenant
    de modifications de la base effectuées par la récupération. C'est parce que la récupération suit
    le mécanisme normal de contrôle de la concurrence, aussi connu sous le nom de <acronym>MVCC</acronym>. 
    Il y a des types de modifications qui causeront des conflits, couverts par la section suivante.
   </para>
  </sect2>

  <sect2 id="hot-standby-conflict">
   <title>Gestion des conflits avec les requêtes</title>

   <para>
    Les noeuds primaire et standby sont de bien des façons faiblement couplés. Des
    actions sur le primaire auront un effet sur le standby. Par conséquent, il y a
    un risque d'interactions négatives ou de conflits entre eux. Le conflit le
    plus simple à comprendre est la performance : si un gros chargement de données a
    lieu sur le primaire, il générera un flux similaire d'enregistrements WAL sur le
    standby, et les requêtes du standby pourrait entrer en compétition pour les ressources
    systèmes, comme les entrées-sorties.
   </para>

   <para>
    Il y a aussi d'autres types de conflits qui peuvent se produire avec le Hot Standby.
    Ces conflits sont des <emphasis>conflits durs</emphasis> dans le sens où des requêtes
    pourraient devoir être annulées et, dans certains cas, des sessions déconnectées, pour les
    résoudre. L'utilisateur dispose de plusieurs moyens de gérer ces conflits, mais
    il est important de d'abord comprendre les causes possibles de ces conflits:

      <itemizedlist>
       <listitem>
        <para>
         Verrous en Access Exclusive du noeud primaire, incluant les commandes
         <command>LOCK</command> explicites et certaines actions <acronym>LDD</acronym>
        </para>
       </listitem>
       <listitem>
        <para>
         Supprimer un tablespace sur le primaire pendant que des requêtes utilisent
         ce tablespace pour des fichiers temporaires de travail 
         (débordement de <varname>work_mem</varname>)
        </para>
       </listitem>
       <listitem>
        <para>
         Drop de base sur le primaire alors que des utilisateurs sont connectés
         à cette base sur le standby.
        </para>
       </listitem>
       <listitem>
        <para>
         Le standby qui attend plus longtemps que <varname>max_standby_delay</varname>
         pour acquérir un verrou de nettoyage de buffer (buffer cleanup lock).
        </para>
       </listitem>
       <listitem>
        <para>
         Nettoyage de données toujours visible dans l'instantané de la requête en cours.
        </para>
       </listitem>
      </itemizedlist>
   </para>

   <para>
    Certaines actions de WAL à rejouer concerneront des exécutions de <acronym>LDD</acronym>.
    Ces actions LDD rejouent des modifications déjà validées sur le noeud primaire,
    elles ne doivent donc pas échouer sur le noeud de standby. Ces verrous LDD pris
    sont prioritaires et *annuleront* automatiquement toute transaction en lecture seule
    qui se mettront en travers de leur chemin, après une période de grâce. C'est similaire
    à l'éventualité d'être annulé par le détecteur de deadlock. Mais dans ce cas particulier,
    le processus de récupération du standby gagne toujours, puisque les actions à rejouer ne
    doivent pas échouer. Cela permet aussi de garantir que la réplication ne prend pas
    de retard en attendant la fin de l'exécution d'une requête. Cette mise en priorité
    part de l'hypothèse que le standby existe principalement pour la haute disponibilite,
    et que l'ajustement de la période de grâce permettra une protection suffisante contre
    les annulations non souhaitées.
   </para>

   <para>
    Un exemple de ce qui précède serait un administrateur qui exécuterait
    <command>DROP TABLE</command> sur une table du serveur primaire qui serait
    en cours d'interrogation sur le standby.
    Il est évident que la requête ne peut pas continuer si <command>DROP TABLE</command> 
    s'exécute. Si cette situation se présentait sur le primaire, le <command>DROP TABLE</command> 
    attendrait jusqu'à ce que la requête soit terminée. Quand <command>DROP TABLE</command> 
    s'exécute sur le primaire, le primaire n'a pas d'information sur les requêtes en cours
    d'exécution sur le standby, et ne peut donc pas attendre les requêtes du standby. Les
    enregistrements de modification WAL sont propagées sur le standby alors que la requête est
    toujours en cours d'exécution, entraînant un conflit.
   </para>

   <para>
    La raison la plus fréquente de conflit entre des requêtes de standby et l'application
    de WAL est le "nettoyage précoce". En temps normal, <productname>PostgreSQL</productname> 
    permet le nettoyage des vieilles versions d'enregistrements quand il n'y a pas d'utilisateur
    qui en ont besoin pour assurer une visibilité correcte des données (c'est le coeur de MVCC).
    S'il y a une requête sur le standby qui s'est exécuté pendant plus longtemps que n'importe
    quelle requête du primaire, il est alors possible que d'anciennes versions d'enregistrements
    soient supprimées par vacuum ou par HOT. Cela générera des enregistrements WAL qui, si
    appliqués, supprimeraient des enregistrements sur le standby qui pourraient 
		<emphasis>potentiellement</emphasis> être nécessaires aux requêtes du standby.
    En langage plus technique, l'horizon xmin du primaire est postérieur à l'horizon xmin
    du standby, permettant aux enregistrements morts d'être supprimés.
   </para>

   <para>
    Les utilisateurs expérimentés devraient noter que le nettoyage et le gel (freeze)
    de versions d'enregistrements pourraient tous les deux être en conflit avec les requêtes
    du standby. Lancer un <command>VACUUM FREEZE</command> manuel a de fortes chances
    de causer des conflits même sur des tables n'ayant pas d'enregistrements modifiés ou supprimés.
    </para>

   <para>
    Il existe plusieurs options pour résoudre les conflits entre requêtes. Celle
    par défaut est d'attendre, et espérer que la requête se termine. Le serveur attendra
    automatiquement jusqu'à ce que le retard du primaire sur le secondaire
    soit au plus de <xref linkend="guc-max-standby-delay"/> (30 secondes par défaut).
    Une fois que la période de grâce est terminée,
    une des actions suivantes est effectuée:

      <itemizedlist>
       <listitem>
        <para>
         Si le conflit est causé par un verrou, la transaction en conflit sur
         le standby est annulée immédiatement. Si la transaction est idle-in-transaction,
         alors la session est annulée immédiatement à la place.
         Ce comportement est sujet à changement ultérieur.
        </para>
       </listitem>

       <listitem>
        <para>
         Si le conflit est causé par des enregistrements de nettoyage, la requête de
         standby est informée qu'un conflit s'est produit et qu'elle doit s'annuler
         pour éviter le risque d'échouer sans retourner d'erreur, parce que les données
         auraient été supprimées. (Ceci est malheureusement similaire au message 
         "snapshot too old", si redouté et si caractéristique). Certain enregistrements
         de nettoyage ne déclenchent de conflit qu'avec de vieilles requêtes, alors que
         d'autres peuvent affecter toutes les requêtes.
        </para>

        <para>
         Si une annulation se produit, la requête et/ou la transaction peut toujours
         être ré-exécutée. L'erreur est dynamique et ne se reproduira pas forcément 
         si la requête est exécutée à nouveau.
        </para>
       </listitem>
      </itemizedlist>
   </para>

   <para>
    Gardez à l'esprit que <varname>max_standby_delay</varname> est comparé à
    la différence entre l'horloge du serveur de standby et l'horodatage des commit
    lus du WAL. Ainsi, la période de grâce permise à n'importe quelle requête sur
    la base de standby n'est jamais supérieure à <varname>max_standby_delay</varname>, 
    et pourrait être considérablement inférieure si le standby est déjà en retard
    consécutivement à l'attente de la fin d'autre requêtes, n'est pas capable
    de suivre une charge importante de mises à jour.
   </para>

   <caution>
    <para>
     Soyez sûr que les horloges des serveurs primaire et de standby sont maintenues synchrones;
     sinon les valeurs comparées à <varname>max_standby_delay</varname> seront erronées,
     entraînant des annulations de requêtes non souhaitables.
     Si les horloges sont volontairement désynchronisées, ou si il y a un grand temps de
     propagation du primaire vers le standby, il est envisageable de positionner
     <varname>max_standby_delay</varname> à -1. Dans tous les cas cette valeur
     devrait être plus grande que le plus grand écart temporel attendu entre
     le primaire et le standby.
    </para>
   </caution>

   <para>
    Les utilisateurs doivent prendre conscience que les tables qui sont régulièrement
    et fortement mises à jour sur le primaire vont rapidement entraîner l'annulation
    de requêtes plus longues sur le standby. Dans ces cas, <varname>max_standby_delay</varname>
    peut être vu comme un paramètre similaire à <varname>statement_timeout</varname>.
    </para>

   <para>
    D'autres méthodes de contournement existent si le nombre d'annulations
    est inacceptable. La première est de se connecter sur le primaire et
    de garder une requête active pendant aussi longtemps que nécessaire pour exécuter
    des requêtes sur le standby. Ceci garantit qu'aucun enregistrement d'annulation
    n'est généré dans les WAL et qu'aucun conflit de requête ne se produira comme décrit
    ci-dessus. Ceci pourrait être effectué via <filename>contrib/dblink</filename>
    et <function>pg_sleep()</function>, ou d'autres mécanismes. Si vous faîtes ceci,
    vous devriez noter que cela repoussera le nettoyage des enregistrements morts sur
    le primaire par vacuum ou HOT, ce que certaines personnes pourraient trouver indésirable.
    Toutefois, rappelez vous que les noeuds primaire et de standby sont liés via le WAL, ce qui
    implique que cette problématique de nettoyage n'est pas différente du cas où la requête
    aurait été exécutée sur le primaire lui-même. Et vous gardez le bénéfice de décharger
    le travail d'exécution sur le standby. <varname>max_standby_delay</varname> ne devrait
    pas être utilisé dans ce cas, parce que les fichiers WAL retardés pourraient déjà contenir
    des entrées qui invalident le snapshot actuel.
   </para>

   <para>
    Il est aussi possible de paramétrer <varname>vacuum_defer_cleanup_age</varname> sur le primaire
    pour retarder le nettoyage des enregistrements par autovacuum, <command>VACUUM</command>
    et HOT. Cela pourrait donner davantage de temps aux requêtes pour s'exécuter avant d'être annulées
    sur le standby, sans avoir besoin de positionner une valeur élevée de <varname>max_standby_delay</varname>.
   </para>

   <para>
    Des deadlocks triples sont possibles entre des <literal>AccessExclusiveLocks</literal> provenant
    du primaire, des enregistrements de nottoyage du WAL nécessitant des verrous de nettoyage, et
    des requêtes qui attendent après des <literal>AccessExclusiveLocks</literal> en cours d'application.
    Les deadlocks sont résolus automatiquement après <varname>deadlock_timeout</varname> secondes,
    mais il est estimé qu'ils seront très rares en pratique.
   </para>

   <para>
    La suppression de tablespaces ou de bases de données sont traitées dans
    la section administrateur puisqu'elles ne constituent pas des cas classiques
    pour les utilisateurs.
   </para>
  </sect2>

  <sect2 id="hot-standby-admin">
   <title>Aperçu pour l'administrateur</title>

   <para>
    Si <varname>hot_standby</varname> est positionné à <literal>on</literal> dans
    <filename>postgresql.conf</filename>  et qu'une fichier <filename>recovery.conf</filename>
    est présent, le serveur fonctionnera en mode Hot Standby.
    Toutefois, il pourrait s'écouler du temps avant que les connections en
    Hot Standby soient autorisées, parce que le serveur n'acceptera pas de connexions tant
    que la récupération n'aura pas atteint un point garantissant un état cohérent permettant
    aux requêtes de s'exécuter. Pendant cette période, les clients qui tentent de se connecter
    seront rejetés avec un message d'erreur.
    Pour confirmer que le serveur a démarré, vous pouvez soit tenter de vous connecter en
    boucle, ou rechercher ces messages dans les journaux du serveur:

<programlisting>
LOG:  entering standby mode

... puis, plus loin ...

LOG:  consistent recovery state reached
LOG:  database system is ready to accept read only connections
</programlisting>

    L'information sur la cohérence est enregistrée une fois par checkpoint sur le primaire.
    Il n'est pas possible d'activer le hot standby si on lit des WAL générés durant
    une période pendant laquelle <varname>wal_level</varname> n'était pas positionné
    à <literal>hot_standby</literal> sur le primaire. L'arrivée à un état cohérent
    peut aussi être retardée si ces deux conditions se présentent:

      <itemizedlist>
       <listitem>
        <para>
         Une transaction en écriture a plus de 64 sous-transactions
        </para>
       </listitem>
       <listitem>
        <para>
         Des transactions en écriture ont une durée très importante
        </para>
       </listitem>
      </itemizedlist>

    Si vous effectuez du log shipping par fichier ("warm standby"), vous pourriez
    devoir attendre jusqu'à l'arrivée du prochain fichier de WAL, ce qui pourrait
    être aussi long que le paramètre <varname>archive_timeout</varname> du primaire.
   </para>

   <para>
    Certains paramètres sur le standby vont devoir être revus si ils ont été modifiés
    sur le primaire. Pour ces paramètres, la valeur sur le standby devra
    être égale ou supérieure à celle du primaire. Si ces paramètres ne sont pas
    suffisamment élevés le standby refusera de démarrer. Il est tout à fait possible
    de fournir de nouvelles valeurs plus élevées et de redémarrer le serveur pour reprendre
    la récupération. Ces paramètres sont les suivants:

      <itemizedlist>
       <listitem>
        <para>
         <varname>max_connections</varname>
        </para>
       </listitem>
       <listitem>
        <para>
         <varname>max_prepared_transactions</varname>
        </para>
       </listitem>
       <listitem>
        <para>
         <varname>max_locks_per_transaction</varname>
        </para>
       </listitem>
      </itemizedlist>
   </para>

   <para>
    Il est important que l'administrateur s'interroge sur le paramétrage approprié
    de <varname>max_standby_delay</varname>,
    qui peut être mis en place via <filename>postgresql.conf</filename>.
    Il n'y a pas de paramètre optimal, il devra donc être réglés en fonction des priorités
    applicatives. Par exemple, si le serveur est principalement voué à la Haute Diponibilité,
    vous voudrez peut être réduire <varname>max_standby_delay</varname> , voire le positionner
    à zéro, bien que ce paramétrage soit très agressif. Si le serveur de standby est voué à
    traiter des requêtes décisionnelles, vous pourriez le positionner à plusieurs heures.
    Il est aussi possible de positionner <varname>max_standby_delay</varname> à -1, ce qui
    signifie d'attendre quoi qu'il arrive que les requêtes se terminent; ceci sera utile
    dans le cas d'une récupération d'une archive à partir d'une sauvegarde.
   </para>

   <para>
    Les "hint bits" (bits d'indices) écrits sur le primaire ne sont pas journalisés en WAL,
    il est donc probable que les les hint bits soient réécrits sur le standby. Ainsi,
    le serveur de standby fera toujours des écritures disques même si tous les utilisateurs
    sont en lecture seule; aucun changement ne se produira sur les données elles mêmes.
    Les utilisateurs écriront toujours les fichiers temporaires pour les gros tris et
    re-génèreront les fichiers d'information relcache, il n'y a donc pas de morceau de la base
    qui soit réellement en lecture seule en mode hot standby.
    Notez aussi que les écritures dans des bases distantes en utilisant le module
    <application>dblink</application> , et d'autres opération en dehors de la base s'appuyant sur
    des fonctions PL seront toujours possibles, même si la transaction est en lecture seule localement.
   </para>

   <para>
    Les types suivants de commandes administratives ne sont pas acceptées
    durant le mode de récupération:

      <itemizedlist>
       <listitem>
        <para>
         Langage de Définition de Données (LDD ou DDL) - comme <command>CREATE INDEX</command>
        </para>
       </listitem>
       <listitem>
        <para>
         Privilège et possession - <command>GRANT</command>, <command>REVOKE</command>,
         <command>REASSIGN</command>
        </para>
       </listitem>
       <listitem>
        <para>
         Commandes de maintenance - <command>ANALYZE</command>, <command>VACUUM</command>,
         <command>CLUSTER</command>, <command>REINDEX</command>
        </para>
       </listitem>
      </itemizedlist>
   </para>

   <para>
    Notez encore une fois que certaines de ces commandes sont en fait
    autorisées durant les transactions en "lecture seule" sur le primaire.
   </para>

   <para>
    Par conséquent, vous ne pouvez pas créer d'index supplémentaires qui existeraient
    uniquement sur le standby, ni des statistiques qui n'existeraient que sur le standby.
    Si ces commandes administratives sont nécessaires, elles doivent être exécutées
    sur le primaire, et ces modifications se propageront à terme au standby.
   </para>

   <para>
    <function>pg_cancel_backend()</function> fonctionnera sur les processus utilisateurs, mais pas sur
    les processus de démarrage, qui effectuent la récupération. <structname>pg_stat_activity</structname> 
    ne montre pas d'entrée pour le processus de démarrage, et les transactions de récupération
    ne sont pas affichées comme actives. Ainsi, <structname>pg_prepared_xacts</structname> est toujours
    vide durant la récupération. Si vous voulez traiter des transactions préparées douteuses,
    interrogez <structname>pg_prepared_xacts</structname>  sur le primaire, et exécutez les commandes
    pour résoudre le problème à cet endroit.
   </para>

   <para>
    <structname>pg_locks</structname> affichera les verrous possédés par les processus,
    comme en temps normal. <structname>pg_locks</structname> affiche aussi une transaction
    virtuelle gérée par le processus de démarrage qui possède tous les
    <literal>AccessExclusiveLocks</literal> possédés par les transactions rejouées par la récupération.
    Notez que le processus de démarrage n'acquiert pas de verrou pour effectuer les modifications à
    la base, et que par conséquent les verrous autre que <literal>AccessExclusiveLocks</literal> 
    ne sont pas visibles dans <structname>pg_locks</structname> pour le processus de démarrage;
    ils sont simplement censés exister.
   </para>

   <para>
    Le plugin <productname>Nagios</productname> <productname>check_pgsql</productname> fonctionnera,
    parce que les informations simples qu'il vérifie existent.
    Le script de supervision <productname>check_postgres</productname> fonctionnera aussi,
    même si certaines valeurs retournées pourraient être différentes ou sujettes à confusion.
    Par exemple, la date de dernier vacuum ne sera pas mise à jour, puisqu'aucun vacuum ne se déclenche
    sur le standby. Les vacuums s'exécutant sur le primaire envoient toujours leurs modifications
    au standby.
   </para>

   <para>
    Les options de contrôle des fichiers de WAL ne fonctionneront pas durant la récupération,
    comme <function>pg_start_backup</function>, <function>pg_switch_xlog</function>, etc...
   </para>

   <para>
    Les modules à chargement dynamique fonctionnent, comme <structname>pg_stat_statements</structname>.
   </para>

   <para>
    Les verrous consultatifs fonctionnent normalement durant la récupération,
    y compris en ce qui concerne la détection des verrous mortels (deadlocks).
    Notez que les verrous consultatifs ne sont jamais tracés dans les WAL, il est
    donc impossible pour un verrou consultatif sur le primaire ou le standby
    d'être en conflit avec la ré-application des WAL. Pas plus qu'il n'est
    possible d'acquérir un verrou consultatif sur le primaire et que celui-ci
    initie un verrou consultatif similaire sur le standby. Les verrous consultatifs
    n'ont de sens que sur le serveur sur lequel ils sont acquis.
   </para>

   <para>
    Les systèmes de réplications à base de triggers tels que <productname>Slony</productname>,
    <productname>Londiste</productname> et <productname>Bucardo</productname>
    ne fonctionneront pas sur le standby du tout, même s'ils fonctionneront sans problème
    sur le serveur primaire tant que les modifications ne sont pas envoyées sur le serveur standby
    pour y être appliquées. Le rejeu de WAL n'est pas à base de triggers, vous ne pouvez
    donc pas utiliser le standby comme relai vers un système qui aurait besoin d'écritures supplémentaires
    ou utilise des triggers.
   </para>

   <para>
    Il n'est pas possible d'assigner de nouveaux oids, bien que des générateurs d' <acronym>UUID</acronym> 
    puissent tout de même fonctionner, tant qu'ils n'ont pas besoin d'écrire un nouveau statut dans
    la base.
   </para>

   <para>
    À l'heure actuelle, la création de table temporaire n'est pas autorisée durant les
    transactions en lecture seule, certains scripts existants pourraient donc
    ne pas fonctionner correctement. Cette restriction pourrait être levée dans une
    version ultérieure. Il s'agit à la fois d'un problème de respect des standards
    et un problème technique.
   </para>

   <para>
    <command>DROP TABLESPACE</command> ne peut réussir que si le tablespace est vide.
    Certains utilisateurs pourraient utiliser de façon active le tablespace via leur
    paramètre <varname>temp_tablespaces</varname>. S'il y a des fichiers temporaires
    dans le tablespace, toutes les requêtes actives sont annulées pour s'assurer que les
    fichiers temporaires sont supprimés, afin de supprimer le tablespace et de continuer
    l'application des WAL.
   </para>

   <para>
    Exécuter <command>DROP DATABASE</command>, <command>ALTER DATABASE ... SET TABLESPACE</command>,
    ou <command>ALTER DATABASE ... RENAME</command> sur le primaire génèrera un message de journal
    qui déconnectera de façon forcée les utilisateurs connectés à cette base sur le standby. Cette
    action est à effet immédiat, quelle que soit la valeur de <varname>max_standby_delay</varname>.
   </para>

   <para>
    En fonctionnement normal (pas en récupération), si vous exécutez 
    <command>DROP USER</command> ou <command>DROP ROLE</command>
    pour un rôle ayant le privilège LOGIN alors que cet utilisateur est toujours
    connecté alors rien ne se produit pour cet utilisateur connecté - il reste connecté. L'utilisateur
    ne peut toutefois pas se reconnecter. Ce comportement est le même en récupération, un
    <command>DROP USER</command> sur le primaire ne déconnecte donc pas cet utilisateur sur le standby.
   </para>

   <para>
    Le collecteur de statistiques est actif durant la récupération. Tous les parcours,
    lectures, utilisations de blocs et d'index, etc... seront enregistrés normalement
    sur le standby. Les actions rejouées ne dupliqueront pas leur effets sur le primaire,
    l'application d'insertions n'incrémentera pas la colonne Inserts de pg_stat_user_tables.
    Le fichier de statistiques est effacé au démarrage de la récupération, les statistiques
    du primaire et du standby différeront donc; c'est vu comme une fonctionnalité, pas un bug.
   </para>

   <para>
    Autovacuum n'est pas actif durant la récupération, il démarrera normalement
    à la fin de la récupération.
   </para>

   <para>
    Le processus d'écriture en arrière plan (background writer) est actif durant
    la récupération et effectuera les restartpoints (points de reprise)
    (similaires aux points de synchronisation ou checkpoints sur le primaire) et
    les activités normales de nettoyage de blocs. Ceci peut inclure la mise à jour
    des information de hint bit des données du serveur de standby.
    La commande <command>CHECKPOINT</command>  est acceptée pendant la récupération,
    bien qu'elle déclenche un restartpoint et non un checkpoint.
   </para>
  </sect2>

  <sect2 id="hot-standby-parameters">
   <title>Référence des paramètres de Hot Standby</title>

   <para>
    De nombreux paramètres ont été mentionnés ci dessus dans 
    <xref linkend="hot-standby-admin"/>
    et <xref linkend="hot-standby-conflict"/>.
   </para>

   <para>
    Sur le primaire, les paramètres <xref linkend="guc-wal-level"/> et
    <xref linkend="guc-vacuum-defer-cleanup-age"/> peuvent être utilisés.
    <xref linkend="guc-max-standby-delay"/> n'a aucun effet sur le primaire.
   </para>

   <para>
    Sur le standby, les paramètres <xref linkend="guc-hot-standby"/> et
    <xref linkend="guc-max-standby-delay"/> peuvent être utilisés.
    <xref linkend="guc-vacuum-defer-cleanup-age"/> n'a aucun effet durant la récupération.
   </para>
  </sect2>

  <sect2 id="hot-standby-caveats">
   <title>Avertissements</title>

   <para>
    Il y a plusieurs limitations de Hot Standby.
    Elles peuvent et seront probablement résolues dans des versions ultérieures:

  <itemizedlist>
   <listitem>
    <para>
     Les opérations sur les index hash ne sont pas écrits dans la WAL à l'heure
     actuelle, la récupération ne mettra donc pas ces index à jour. Les index
     hash ne seront pas utilisés pour les plans d'exécution durant la récupération.
    </para>
   </listitem>
   <listitem>
    <para>
     Une connaissance complète des transactions en cours d'exécution est nécessaire
     avant de pouvoir déclencher des instantanés. Des transactions utilisant un
     grand nombre de sous-transactions (à l'heure actuelle plus de 64) retarderont
     le démarrage des connexions en lecture seule jusqu'à complétion de la plus
     longue transaction en écriture. Si cette situation se produit, des messages
     explicatifs seront envoyés dans la trace du serveur.
    </para>
   </listitem>
   <listitem>
    <para>
     Des points de démarrage valides pour les requêtes de standby sont générés
     à chaque checkpoint sur le maître. Si le standby est éteint alors
     que le maître est déjà éteint, il est tout à fait possible ne ne pas pouvoir
     repasser en Hot Standby tant que le primaire n'aura pas été redémarré, afin
     qu'il génère de nouveaux points de démarrage dans les journaux WAL. Cette situation
     n'est pas un problème dans la plupart des situations où cela pourrait se produire.
     Généralement, si le primaire est éteint et plus disponible, c'est probablement
     en raison d'un problème sérieux qui va de toutes façons forcer la conversion
     du standby en primaire. Et dans des situations où le primaire est éteint
     intentionnellement, la procédure standard est de promouvoir le maître.
    </para>
   </listitem>
   <listitem>
    <para>
     À la fin de la récupération, les <literal>AccessExclusiveLocks</literal> possédés
     par des transactions préparées nécessiteront deux fois le nombre d'entrées normal dans la
     table de verrous. Si vous pensez soit exécuter un grand nombre de transactions préparées
     prenant des <literal>AccessExclusiveLocks</literal>, ou une grosse transaction prenant
     beaucoup de <literal>AccessExclusiveLocks</literal>, il est conseillé d'augmenter la valeur
     de <varname>max_locks_per_transaction</varname>, peut-être jusqu'à une valeur double
     de celle du serveur primaire. Vous n'avez pas besoin de prendre ceci en compte
     si votre paramètre <varname>max_prepared_transactions</varname> est <literal>0</literal>.
    </para>
   </listitem>
   
   
   
   
   
   
   
   
  </itemizedlist>

   </para>
  </sect2>

 </sect1>

  <sect1 id="backup-incremental-updated">
   <title>Sauvegardes mises à jour de façon incrémentielle</title>

  <indexterm zone="high-availability">
   <primary>sauvegardes mises à jour de façon incrémentielle</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>accumulation de modifications</primary>
  </indexterm>

   <para>
    Dans une configuration de standby, il est possible de décharger le serveur 
    primaire du coût des sauvegardes de base périodiques; à la place, les
    sauvegardes de base peuvent être réalisées en sauvegardant les fichiers du
    serveur de standby. Ce concept est habituellement connu sous le nom
    de sauvegardes mises à jour de façon incrémentielle, accumulation de
    modifications de journaux, ou plus simplement, accumulation de modifications.
   </para>

   <para>
    Si nous déclenchons une sauvegarde de système de fichier sur les données
    du serveur de standby alors qu'il traite les journaux reçus du primaire,
    nous pourrons recharger cette sauvegarde et redémarrer le processus de récupération
    à partir du dernier restartpoint. Nous n'avons plus besoin des fichiers de WAL
    d'avant le restartpoint du standby. Si une récupération est nécessaire, il
    sera plus rapide de récupérer à partir de la sauvegarde mise à jour incrémentiellement
    que de la sauvegarde de base original.
   </para>

   <para>
    La procédure pour effectuer une sauvegarde de fichier des données du standby
    alors qu'il traite les journaux provenant du primaire est:
   <orderedlist>
    <listitem>
     <para>
      Effectuer la sauvegarde, sans utiliser <function>pg_start_backup</function> et
      <function>pg_stop_backup</function>. Notez que le fichier <filename>pg_control</filename>
      doit être sauvegardé en <emphasis>premier</emphasis>, comme ici:
      
<programlisting>
cp /var/lib/pgsql/data/global/pg_control /tmp
cp -r /var/lib/pgsql/data /chemin/vers/sauvegarde
mv /tmp/pg_control /chemin/vers/sauvegarde/data/global
</programlisting>
      <filename>pg_control</filename> contient l'endroit où le rejeu du WAL commencera
      après avoir restauré à partir de la sauvegarde; le sauvegarder en premier
      garantit qu'il pointe vers le dernier restartpoint quand la sauvegarde a
      commencé, pas un restartpoint ultérieur au moment où les fichiers ont
      été copiés vers la sauvegarde.
     </para>
    </listitem>
    <listitem>
     <para>
      Notez l'entrée de WAL de la fin de la sauvegarde en appelant
      la fonction <function>pg_last_xlog_replay_location</function> à la
      fin de la sauvegarde, et en la gardant avec la sauvegarde.
<programlisting>
psql -c "select pg_last_xlog_replay_location();" > /chemin/vers/sauvegarde/position_finale
</programlisting>
      Lors d'une récupération à partir d'une sauvegarde mise à jour incrémentiellement,
      le serveur peut accepter les connexions et terminer la récupération avec succès
      avant que la base ne soit devenue cohérente. Pour éviter ceci, vous devez
      vous assurer que la base est cohérente avant que les utilisateurs n'essaient
      de se connecter au serveur et quand la récupération se termine. Vous pouvez le
      faire en comparant l'avancement de la récupération avec la position de WAL finale
      de la sauvegarde que vous avez conservée: le serveur n'est pas cohérent avant
      que la récupération n'ait atteint la position de WAL finale de la sauveqarde.
      L'avancement de la récupération peut aussi être surveillée avec la fonction
      <function>pg_last_xlog_replay_location</function>, mais cela impose de se
      connecter au serveur alors qu'il n'est pas encore cohérent. Il faut donc être
      prudent avec cette méthode.
     </para>
     <para>
     </para>
    </listitem>
   </orderedlist>
   </para>

   <para>
    Puisque le serveur de standby n'est pas <quote>actif</quote>, il n'est pas possible
    d'utiliser <function>pg_start_backup()</function> et<function>pg_stop_backup()</function>
    pour gérer le processus de sauvegarde; ce sera à vous de déterminer quelle quantité de
    fichiers de segments WAL garder pour avoir une sauvegarde récupérable. Ceci est déterminé
    par le dernier restartpoint au moment de la sauvegarde, tout WAL plus ancien que cela
    peut être supprimé de l'archive une fois que la sauvegarde a été réalisée. Vous pouvez
    déterminer le dernier restartpoint en exécutant <application>pg_controldata</application> 
    sur le serveur de standby avant de commencer la sauvegarde, ou en positionnant l'option
    <varname>log_checkpoints</varname> pour écrire les valeurs dans le journal applicatif du
    serveur de standby.
   </para>
  </sect1>

</chapter>
