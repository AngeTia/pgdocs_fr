<?xml version="1.0" encoding="ISO-8859-15"?>
<!-- Dernière modification
     le       $Date$
     par      $Author$
     révision $Revision$ -->

<chapter id="high-availability">
 <title>Haute disponibilité, répartition de charge et réplication</title>

 <indexterm><primary>haute disponibilité</primary></indexterm>
 <indexterm><primary>failover</primary></indexterm>
 <indexterm><primary>réplication</primary></indexterm>
 <indexterm><primary>répartition de charge</primary></indexterm>
 <indexterm><primary>clustering</primary></indexterm>
 <indexterm><primary>partitionnement de données</primary></indexterm>

<!-- seamlessly ? -->
 <para>
  Des serveurs de bases de données peuvent travailler ensemble pour permettre
  à un serveur secondaire de prendre rapidement la main si le serveur principal
  échoue (haute disponibilité, ou <foreignphrase>high availability</foreignphrase>),
  ou pour permettre à plusieurs serveurs de servir les mêmes données (répartition
  de charge, ou <foreignphrase>load balancing</foreignphrase>). Idéalement, les
  serveurs de bases de données peuvent travailler ensemble sans jointure.
 </para>
  
 <para>
  Il est aisé de faire coopérer des serveurs web qui traitent des pages web statiques
  en répartissant la charge des requêtes web sur plusieurs
  machines. Dans les faits, les serveurs de bases de données en lecture seule peuvent
  également coopérer facilement. Malheureusement, la plupart des
  serveurs de bases de données traitent des requêtes de lecture/écriture et,
  de ce fait, collaborent plus difficilement. En effet, alors qu'il suffit de
  placer une seule fois les données en lecture seule sur chaque serveur, une
  écriture sur n'importe quel serveur doit, elle, être propagée à tous les
  serveurs afin que les lectures suivantes sur ces serveurs renvoient des résultats
  cohérents.
 </para>

 <para>
  Ce problème de synchronisation représente la difficulté fondamentale à la
  collaboration entre serveurs. Comme la solution au problème de
  synchronisation n'est pas unique pour tous les cas pratiques, plusieurs
  solutions co-existent. Chacune répond de façon différente et minimise
  cet impact au regard d'une charge spécifique.
 </para>

 <para>
  Certaines solutions gèrent la synchronisation en autorisant les modifications
  des données sur un seul serveur. Les serveurs qui peuvent modifier les données
  sont appelés serveur en lecture/écriture, <firstterm>maître</firstterm> ou serveur <firstterm>primaire</firstterm>.
  Les serveurs qui suivent les modifications du maître sont appelés <firstterm>standby</firstterm>,
  ou serveurs <firstterm>esclaves</firstterm>. Un serveur en standby auquel on ne peut pas
  se connecter tant qu'il n'a pas été promu en serveur maître est appelé un serveur en <firstterm>warm
  standby</firstterm>, et un qui peut accepter des connections et répondre à des requêtes en
  lecture seule est appelé un serveur en <firstterm>hot standby</firstterm>.
 </para>

 <para>
  Certaines solutions sont synchrones, ce qui signifie qu'une transaction de
  modification de données n'est pas considérée valide tant que tous les
  serveurs n'ont pas validé la transaction. Ceci garantit qu'un
  <foreignphrase>failover</foreignphrase> ne perd pas de données et que tous
  les serveurs en répartition de charge retournent des résultats cohérents, quel
  que soit le serveur interrogé. Au contraire, les solutions asynchrones
  autorisent un délai entre la validation et sa propagation aux
  autres serveurs. Cette solution implique une éventuelle perte de transactions
  lors de la bascule sur un serveur de sauvegarde, ou l'envoi de données
  obsolètes par les serveurs à charge répartie. La communication asynchrone est
  utilisée lorsque la version synchrone est trop lente.
 </para>

 <para>
  Les solutions peuvent aussi être catégorisées par leur granularité. Certaines
  ne gèrent que la totalité d'un serveur de bases alors que
  d'autres autorisent un contrôle par table ou par base.
 </para>

 <para>
  Il importe de considérer les performances dans tout choix. Il y
  a généralement un compromis à trouver entre les fonctionnalités et les
  performances. Par exemple, une solution complètement synchrone sur un réseau
  lent peut diviser les performances par plus de deux, alors qu'une
  solution asynchrone peut n'avoir qu'un impact minimal sur les performances.
 </para>

 <para>
  Le reste de cette section souligne différentes solutions de
  <foreignphrase>failover</foreignphrase>, de réplication et de répartition de
  charge. Un <ulink
  url="http://www.postgres-r.org/documentation/terms">glossaire</ulink> est
  aussi disponible.
 </para>

 <sect1 id="different-replication-solutions">
 <title>Comparaison de différentes solutions</title>

 <variablelist>

 <varlistentry>
  <term><foreignphrase>Failover</foreignphrase> sur disque partagé</term>
  <listitem>

   <para>
    Le <foreignphrase>failover</foreignphrase> (ou bascule sur incident)
    sur disque partagé élimine la surcharge de synchronisation par
    l'existence d'une seule copie de la base de données. Il utilise un
    seul ensemble de disques partagé par plusieurs serveurs. Si le serveur
    principal échoue, le serveur en attente
    est capable de monter et démarrer la base comme s'il récupérait d'un
    arrêt brutal. Cela permet un <foreignphrase>failover</foreignphrase>
    rapide sans perte de données.
   </para>

   <para>
    La fonctionnalité de matériel partagé est commune aux périphériques de
    stockage en réseau. Il est également possible d'utiliser un système de
    fichiers réseau bien qu'il faille porter une grande attention au système de
    fichiers pour s'assurer qu'il a un comportement <acronym>POSIX</acronym>
    complet (voir <xref linkend="creating-cluster-nfs"/>). Cette méthode
    comporte une limitation significative&nbsp;: si les disques ont un
    problème ou sont corrompus, le serveur primaire et le serveur en attente sont tous
    les deux non fonctionnels. Un autre problème est que le serveur en attente
    ne devra jamais accéder au stockage partagé tant que le serveur principal
    est en cours d'exécution.
   </para>

   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Réplication de système de fichiers (périphérique bloc)</term>
   <listitem>

   <para>
    Il est aussi possible d'utiliser cette fonctionnalité d'une autre façon
    avec une réplication du système de fichiers, où toutes les modifications
    d'un système de fichiers sont renvoyées sur un système de fichiers situé
    sur un autre ordinateur. La seule restriction est que ce miroir doit être
    construit de telle sorte que le serveur en attente dispose d'une
    version cohérente du système de fichiers &mdash; spécifiquement, les
    écritures sur le serveur en attente doivent être réalisées dans le même
    ordre que celles sur le maître. <productname>DRBD</productname> est une
    solution populaire de réplication de systèmes de fichiers pour Linux.
   </para>

<!--
https://forge.continuent.org/pipermail/sequoia/2006-November/004070.html

Oracle RAC is a shared disk approach and just send cache invalidations
to other nodes but not actual data. As the disk is shared, data is
only committed once to disk and there is a distributed locking
protocol to make nodes agree on a serializable transactional order.
-->

  </listitem>
 </varlistentry>

 <varlistentry>
  <term><foreignphrase>Warm et Hot Standby</foreignphrase> en utilisant
    <acronym>PITR</acronym></term>
  <listitem>

   <para>
    Les serveurs <foreignphrase>warm et hot standby</foreignphrase> (voir <xref
    linkend="warm-standby"/>) peuvent conserver leur cohérence en lisant un flux
    d'enregistrements de <acronym>WAL</acronym>. Si le serveur principal
    échoue, le serveur
    <foreignphrase>standby</foreignphrase> contient pratiquement toutes
    les données du serveur principal et peut rapidement devenir le nouveau
    serveur maître. Ceci est asynchrone et ne peut se faire que pour le
    serveur de bases complet.
   </para>
    <para>
     Un serveur de standby en PITR peut être implémenté en utilisant la recopie de journaux par fichier
     (<xref linkend="warm-standby"/>)  ou la streaming replication (réplication en continu, voir 
     <xref linkend="streaming-replication"/>), ou une combinaison des deux. Pour
     des informations sur le hot standby, voyez <xref linkend="hot-standby"/>..
    </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Réplication maître/esclave basé sur des triggers</term>
  <listitem>

   <para>
    Une configuration de réplication maître/esclave envoie toutes les requêtes
    de modification de données au serveur maître. Ce serveur envoie les
    modifications de données de façon asynchrone au serveur esclave. L'esclave
    peut répondre aux requêtes en lecture seule alors que le serveur maître
    est en cours d'exécution. Le serveur esclave est idéal pour les requêtes
    vers un entrepôt de données.
   </para>

   <para>
    <productname>Slony-I</productname> est un exemple de ce type de
    réplication, avec une granularité par
    table et un support des esclaves multiples. Comme il met à jour le serveur
    esclave de façon asynchrone (par lots), il existe une possibilité de perte
    de données pendant un <foreignphrase>failover</foreignphrase>.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term><foreignphrase>Middleware</foreignphrase> de réplication basé sur les
    instructions</term>
  <listitem>

   <para>
    Avec les <foreignphrase>middleware</foreignphrase> de réplication basés
    sur les instructions, un programme intercepte chaque requête SQL et
    l'envoie à un ou tous les serveurs. Chaque serveur opère indépendamment.
    Les requêtes en lecture/écriture sont envoyées à tous les serveurs alors
    que les requêtes en lecture seule ne peuvent être envoyées qu'à un seul
    serveur, ce qui permet de distribuer la charge de lecture.
   </para>

   <para>
    Si les requêtes sont envoyées sans modification, les fonctions comme
    <function>random()</function>, <function>CURRENT_TIMESTAMP</function> ainsi
    que les séquences ont des valeurs différentes sur les différents serveurs.
    Cela parce que chaque serveur opère indépendamment alors que
    les requêtes SQL sont diffusées (et non les données
    modifiées). Si cette solution est inacceptable, le
    <foreignphrase>middleware</foreignphrase> ou l'application doivent
    demander ces valeurs à un seul serveur, et les utiliser dans
    des requêtes d'écriture. Une autre solution est d'utiliser cette solution de réplication
    avec une configuration maître-esclave traditionnelle, c'est à dire que les requêtes
    de modification de données ne sont envoyées qu'au maître et sont propagées aux
    esclaves via une réplication maître-esclave, pas par le middleware de
    réplication.  Il est impératif que
    toute transaction soit validée ou annulée sur tous les serveurs,
    éventuellement par validation en deux phases (<xref
    linkend="sql-prepare-transaction"/> et <xref linkend="sql-commit-prepared"/>.
    <productname>Pgpool-II</productname> et <productname>Sequoia</productname>
    sont des exemples de ce type de réplication.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Réplication asynchrone multi-maîtres</term>
  <listitem>

   <para>
    Pour les serveurs qui ne sont pas connectés en permanence, comme les
    ordinateurs portables ou les serveurs distants, conserver la cohérence des données
    entre les serveurs est un challenge. L'utilisation de la réplication asynchrone
    multi-maîtres permet à chaque serveur de fonctionner indépendamment. Il
    communique alors périodiquement avec les autres serveurs pour identifier les transactions
    conflictuelles. La gestion des conflits est alors confiée aux utilisateurs
    ou à un système de règles de résolution.
		Bucardo est un exemple de ce type de réplication.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Réplication synchrone multi-maîtres</term>
  <listitem>

   <para>
    Dans les réplications synchrones multi-maîtres, tous les serveurs acceptent
    les requêtes en écriture. Les données modifiées sont transmises
    du serveur d'origine à tous les autres serveurs avant toute validation de
    transaction.
   </para>
   <para>
    Une activité importante en écriture peut être la cause d'un
    verrouillage excessif et conduire à un effondrement des performances. Dans
    les faits, les performances en écriture sont souvent pis que celles d'un
    simple serveur.
   </para>
   <para>
    Tous les serveurs acceptent les requêtes en lecture.
   </para>
   <para>
    Certaines implantations utilisent les disques partagés pour réduire la surcharge
    de communication.
   </para>
   <para>
    Les performances de la réplication synchrone multi-maîtres sont meilleures lorsque
    les opérations de lecture représentent l'essentiel de la charge, alors que
    son gros avantage est l'acceptation des requêtes d'écriture par tous les
    serveurs &mdash; 
    il n'est pas nécessaire de répartir la charge entre les serveurs
    maîtres et esclaves et, parce que les modifications de données sont envoyées
    d'un serveur à l'autre, les fonctions non déterministes, comme
    <function>random()</function>, ne posent aucun problème.
   </para>

   <para>
    <productname>PostgreSQL</productname> n'offre pas ce type de réplication,
    mais la validation en deux phases de <productname>PostgreSQL</productname>
    (<xref linkend="sql-prepare-transaction"/> et <xref linkend="sql-commit-prepared"/>)
    autorise son intégration dans une application ou un
    <foreignphrase>middleware</foreignphrase>.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Solutions commerciales</term>
  <listitem>

   <para>
    Parce que <productname>PostgreSQL</productname> est libre et facilement
    extensible, certaines sociétés utilisent <productname>PostgreSQL</productname>
    dans des solutions commerciales fermées
    (<foreignphrase>closed-source</foreignphrase>) proposant des fonctionnalités de
    bascule sur incident (<foreignphrase>failover</foreignphrase>),
    réplication et répartition de charge.
   </para>
  </listitem>
 </varlistentry>

 </variablelist>

 <para>
  La <xref linkend="high-availability-matrix"/> résume les
  possibilités des différentes solutions listées plus-haut.
 </para>

 <table id="high-availability-matrix">
  <title>Matrice de fonctionnalités&nbsp;: haute disponibilité, répartition de
    charge et réplication</title>
  <tgroup cols="8">
   <thead>
    <row>
     <entry>Fonctionnalité</entry>
     <entry>Bascule par disques partagés (<foreignphrase>Shared Disk
     Failover</foreignphrase>)</entry>
     <entry>Réplication par système de fichiers</entry>
     <entry>Secours semi-automatique (<foreignphrase>Hot/Warm
     Standby</foreignphrase>) par <acronym>PITR</acronym></entry>
     <entry>Réplication maître/esclave basé sur les triggers</entry>
     <entry><foreignphrase>Middleware</foreignphrase> de réplication
       sur instructions</entry>
     <entry>Réplication asynchrone multi-maîtres</entry>
     <entry>Réplication synchrone multi-maîtres</entry>
    </row>
   </thead>

   <tbody>

    <row>
     <entry>Exemple d'implémentation</entry>
     <entry align="center">NAS</entry>
     <entry align="center">DRBD</entry>
     <entry align="center">PITR</entry>
     <entry align="center">Slony</entry>
     <entry align="center">pgpool-II</entry>
     <entry align="center">Bucardo</entry>
     <entry align="center"></entry>
    </row>

    <row>
     <entry>Méthode de communication</entry>
     <entry align="center">Disque partagé</entry>
     <entry align="center">Blocs disque</entry>
     <entry align="center">WAL</entry>
     <entry align="center">Lignes de tables</entry>
     <entry align="center">SQL</entry>
     <entry align="center">Lignes de tables</entry>
     <entry align="center">Lignes de tables et verrous de ligne</entry>
    </row>

    <row>
     <entry>Ne requiert aucun matériel spécial </entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Autorise plusieurs serveurs maîtres </entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Pas de surcharge sur le serveur maître </entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
    </row>

    <row>
     <entry>Pas d'attente entre serveurs</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
    </row>

    <row>
     <entry>Pas de perte de données en cas de panne du maître</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Les esclaves acceptent les requêtes en lecture seule</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">Hot only</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Granularité de niveau table</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Ne nécessite pas de résolution de conflit</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
    </row>

   </tbody>
  </tgroup>
 </table>

 <para>
  Certaines solutions n'entrent pas dans les catégories ci-dessus&nbsp;:
 </para>

 <variablelist>

 <varlistentry>
  <term>Partitionnement de données</term>
  <listitem>

   <para>
    Le partitionnement des données divise les tables en ensembles de données.
    Chaque ensemble ne peut être modifié que par un seul serveur. Les
    données peuvent ainsi être partitionnées par bureau, Londres et
    Paris, par exemple, avec un serveur dans chaque bureau. Si certaines
    requêtes doivent combiner des données de Londres et Paris, il est possible
    d'utiliser une application qui requête les deux serveurs ou d'implanter une
    réplication maître/esclave pour conserver sur chaque serveur une copie en lecture
    seule des données de l'autre bureau.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Exécution de requêtes en parallèle sur plusieurs serveurs</term>
  <listitem>

   <para>
    La plupart des solutions ci-dessus permettent à plusieurs serveurs de
    répondre à des requêtes multiples, mais aucune ne permet à une seule requête
    d'être exécutée sur plusieurs serveurs pour se terminer plus rapidement.
    Cette solution autorisent plusieurs serveurs à travailler ensemble sur une
    seule requête. Ceci s'accomplit habituellement en répartissant les données
    entre les serveurs, chaque serveur exécutant une partie de la
    requête pour renvoyer les résultats à un serveur central qui les combine
    et les renvoie à l'utilisateur. <productname>Pgpool-II</productname>
    offre cette possibilité. Cela peut également être implanté en utilisant les
    outils <productname>PL/Proxy</productname>.
   </para>
  </listitem>
 </varlistentry>

 </variablelist>

 </sect1>

 <sect1 id="warm-standby">
 <title>Serveurs de Standby par transfert de journaux</title>


  <para>
   L'archivage en continu peut être utilisé pour créer une configuration
   de cluster en <firstterm>haute disponibilité</firstterm> (HA) avec un ou
   plusieurs <firstterm>serveurs de standby</firstterm> prêts à prendre la main
   sur les opérations si le serveur primaire fait défaut. Cette fonctionnalité
   est généralement appelée 
   <firstterm>warm standby</firstterm> ou <firstterm>log shipping</firstterm>.
  </para>

  <para>
   Les serveurs primaire et de standby travaillent de concert pour fournir cette fonctionnalité,
   bien que les serveurs ne soient que faiblement couplés. Le serveur primaire opère
   en mode d'archivage en continu, tandis que le serveur de standby opère en
   mode de récupération en continu, en lisant les fichiers WAL provenant du primaire. Aucune
   modification des tables de la base ne sont requises pour activer cette fonctionnalité,
   elle entraîne donc moins de travail d'administration par rapport à d'autres
   solutions de réplication. Cette configuration a aussi un impact relativement
   faible sur les performances du serveur primaire.
  </para>

  <para>
   Déplacer directement des enregistrements de WAL d'un serveur de bases de données à un autre
   est habituellement appelé log shipping. <productname>PostgreSQL</productname>
   implémente le log shipping par fichier, ce qui signifie que les enregistrements de WAL sont
   transférés un fichier (segment de WAL) à la fois. Les fichiers de WAL (16Mo) peuvent être
   transférés facilement et de façon peu coûteuse sur n'importe quelle distance, que ce soit sur un
   système adjacent, un autre système sur le même site, ou un autre système à
   l'autre bout du globe. La bande passante requise pour cette technique
   varie en fonction du débit de transactions du serveur primaire.
   Le log shipping par enregistrement est aussi possible avec la streaming replication
   (voir <xref linkend="streaming-replication"/>).
  </para>

  <para>
   Il convient de noter que le log shipping est asynchrone, c'est à dire que les 
   enregistrements de WAL sont transférés après que la transaction ait été validée. Par conséquent, il y a
   un laps de temps pendant lequel une perte de données pourrait se produire si le serveur primaire
   subissait un incident majeur; les transactions pas encore transférées seront perdues. La taille de la fenêtre
   de temps de perte de données peut être réduite par l'utilisation du paramètre
   <varname>archive_timeout</varname>, qui peut être abaissé à des valeurs
   de quelques secondes. Toutefois, un paramètre si bas augmentera de façon 
   considérable la bande passante nécessaire pour le transfert de fichiers.
   Si vous voulez une fenêtre de moins d'une minute environ, envisagez l'utilisation
   de la streaming replication (voir <xref linkend="streaming-replication"/>).
  </para>

  <para>
   La performance de la récupération est suffisamment bonne pour que le standby ne
   soit en général qu'à quelques instants de la pleine
   disponibilité à partir du moment où il aura été activé. C'est pour cette raison que
   cette configuration de haute disponibilité est appelée warm standby.
   Restaurer un serveur d'une base de sauvegarde archivée, puis appliquer tous les journaux
   prendra largement plus de temps, ce qui fait que cette technique est une solution
   de 'disaster recovery' (reprise après sinistre), pas de haute disponibilité.
   Un serveur de standby peut aussi être utilisé pour des requêtes en lecture seule, dans
   quel cas il est appelé un serveur de Hot Standby. Voir <xref linkend="hot-standby"/> pour
   plus d'information.
 </para>

  <indexterm zone="high-availability">
   <primary>warm standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>PITR standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>serveur de standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>log shipping</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>serveur témoin</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>STONITH</primary>
  </indexterm>

  <sect2 id="standby-planning">
   <title>Préparatifs</title>

   <para>
    Il est habituellement préférable de créer les serveurs primaire et de standby
    de façon à ce qu'ils soient aussi similaires que possible, au moins du
    point de vue du serveur de bases de données. En particulier, les chemins
    associés avec les tablespaces seront passés d'un noeud à l'autre sans conversion, ce qui
    implique que les serveurs primaire et de standby doivent avoir les mêmes chemins de montage pour
    les tablespaces si cette fonctionnalité est utilisée. Gardez en tête que si
    <xref linkend="sql-createtablespace"/> 
    est exécuté sur le primaire, tout nouveau point de montage nécessaire pour cela doit être créé
    sur le primaire et tous les standby avant que la commande ne
    soit exécutée. Le matériel n'a pas besoin d'être exactement le même, mais l'expérience monte
    que maintenir deux systèmes identiques est plus facile que maintenir deux
    différents sur la durée de l'application et du système.
    Quoi qu'il en soit, l'architecture hardware doit être la même &mdash; répliquer
    par exemple d'un serveur 32 bits vers un 64 bits ne fonctionnera pas.
   </para>

   <para>
    De manière générale, le log shipping entre serveurs exécutant des versions 
    majeures différentes de <productname>PostgreSQL</productname> est
    impossible. La politique du PostgreSQL Global Development Group est de ne pas
    réaliser de changement sur les formats disques lors des mises à jour mineures,
    il est par conséquent probable que l'exécution de versions mineures différentes
    sur le primaire et le standby fonctionne correctement. Toutefois, il n'y a 
    aucune garantie formelle de cela et il est fortement conseillé de garder le 
    serveur primaire et celui de standby au même niveau de version autant que faire
    se peut. Lors d'une mise à jour vers une nouvelle version mineure, la politique la
    plus sûre est de mettre à jour les serveurs de standby d'abord &mdash; une nouvelle
    version mineure est davantage susceptible de lire les enregistrements WAL d'une
    ancienne version mineure que l'inverse.
   </para>

  </sect2>

  <sect2 id="standby-server-operation">
   <title>Fonctionnement du Serveur de Standby</title>

   <para>
    En mode de standby, le serveur applique continuellement les WAL reçus du
    serveur maître. Le serveur de standby peut lire les WAL d'une archive WAL
    (voir <varname>restore_command</varname>) ou directement du maître via une
    connexion TCP (streaming replication). Le serveur de standby essaiera aussi de
    restaurer tout WAL trouvé dans le répertoire <filename>pg_xlog</filename> du
    cluster de standby. Cela se produit habituellement après un redémarrage de
    serveur, quand le standby rejoue à nouveau les WAL qui ont été reçu du maître
    avant le redémarrage, mais vous pouvez aussi copier manuellement des fichiers dans
    <filename>pg_xlog</filename> à tout moment pour qu'ils soient rejoués.
   </para>

   <para>
    Au démarrage, le serveur de standby commence par restaurer tous les WAL
    disponibles à l'endroit où se trouvent les archives, en appelant la
    <varname>restore_command</varname>. Une fois qu'il a épuisé tous les WAL
    disponibles à cet endroit et que <varname>restore_command</varname>
    échoue, il essaye de restaurer tous les WAL disponibles dans le répertoire
    pg_xlog. Si cela échoue, et que la streaming replication a été activée, le standby essaye
    de se connecter au serveur primaire et de démarrer la réception des WAL depuis
    le dernier enregistrement valide trouvé dans les archives ou pg_xlog. Si cela
    échoue ou que la streaming replication n'est pas configurée, ou que la connexion
    est plus tard déconnectée, le standby retourne à l'étape 1 et essaye de 
    restaurer le fichier à partir de l'archive à nouveau. Cette boucle de
    retentatives de l'archive, pg_xlog et par la streaming replication continue
    jusqu'à ce que le serveur soit stoppé ou que le failover (bascule) soit
    déclenché par un fichier trigger (déclencheur).
   </para>

   <para>
    Le mode de standby est quitté et le serveur bascule en mode de fonctionnement normal
    quand un fichier de trigger est trouvé (<varname>trigger_file</varname>). Avant
    de basculer, tout WAL immédiatement disponible dans l'archive ou le pg_xlog sera
    restaurée, mais aucune tentative ne sera faite pour se connecter au maître.
   </para>
  </sect2>

  <sect2 id="preparing-master-for-standby">
   <title>Préparer le Maître pour les Serveurs de Standby</title>

   <para>
    Mettez en place un archivage en continu sur le primaire vers un répertoire
    d'archivage accessible depuis le standby, comme décrit 
    dans <xref linkend="continuous-archiving"/>. La destination d'archivage devrait être
    accessible du standby même quand le maître est inaccessible, c'est à dire qu'il
    devrait se trouver sur le serveur de standby lui-même ou un autre serveur de confiance, pas sur
    le serveur maître.
   </para>

   <para>
    Si vous voulez utiliser la streaming replication, mettez en place l'authentification sur le
    serveur primaire pour autoriser les connexions de réplication à partir du (des) serveur de
    standby; c'est à dire, mettez en place une ou des entrées appropriées dans
    <filename>pg_hba.conf</filename> avec le champ database positionné à
    <literal>replication</literal>. Vérifiez aussi que <varname>max_wal_senders</varname> est positionné
    à une valeur suffisamment grande dans le fichier de configuration du serveur primaire.
   </para>

   <para>
    Effectuez une sauvegarde de base comme décrit dans <xref linkend="backup-base-backup"/>
    pour initialiser le serveur de standby.
   </para>
  </sect2>

  <sect2 id="standby-server-setup">
   <title>Paramétrer un Serveur de Standby</title>

   <para>
    Pour paramétrer le serveur de standby, restaurez la sauvegarde de base effectué sur
    le serveur primaire (voir (see <xref linkend="backup-pitr-recovery"/>). Créez un
    fichier de commande de récupération <filename>recovery.conf</filename> dans
    le répertoire de données du cluster de standby, et positionnez <varname>standby_mode</varname>
    à on. Positionnez <varname>restore_command</varname> à une simple commande qui recopie
    les fichiers de l'archive de WAL.
   </para>

   <note>
     <para>
     N'utilisez pas pg_standby ou des outils similaires avec le mode de standby intégré
     décrit ici. <varname>restore_command</varname> devrait retourner immédiatement
     si le fichier n'existe pas; le serveur essayera la commande à nouveau si nécessaire.
     Voir <xref linkend="log-shipping-alternative"/> pour utiliser des outils tels que pg_standby.
    </para>
   </note>

   <para>
     Si vous souhaitez utiliser la streaming replication, renseignez
     <varname>primary_conninfo</varname> avec une chaîne de connexion libpq,
     contenant le nom d'hôte (ou l'adresse IP) et tout détail supplémentaire
     nécessaire pour se connecter au serveur primaire. Si le primaire a besoin d'un
     mot de passe pour l'authentification, le mot de passe doit aussi être spécifié dans
     <varname>primary_conninfo</varname>.
   </para>

   <para>
    Vous pouvez utiliser <varname>archive_cleanup_command</varname> pour supprimer de
    l'archive les fichiers qui ne sont plus nécessaires à la base de standby.
   </para>

   <para>
    Si vous mettez en place le serveur de standby pour des besoins de haute disponibilité,
    mettez en place l'archivage de WAL, les connexions et l'authentification à l'identique
    du serveur primaire, parce que le serveur de standby fonctionnera comme un serveur primaire
    après la bascule. Vous aurez aussi besoin de positionner <varname>trigger_file</varname>
    pour rendre la bascule possible.
    Si vous mettez en place le serveur de standby pour des besoins de reporting, sans aucune
    intention de basculer dessus, <varname>trigger_file</varname> n'est pas nécessaire.
   </para>

   <para>
    Un simple exemple de <filename>recovery.conf</filename> est:
<programlisting>
standby_mode = 'on'
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
restore_command = 'cp /path/to/archive/%f %p'
trigger_file = '/path/to/trigger_file'
</programlisting>
   </para>

   <para>
    Vous pouvez avoir n'importe quel nombre de serveurs de standby, mais si vous
    utilisez la streaming replication, assurez vous d'avoir positionné
    <varname>max_wal_senders</varname> suffisamment haut sur le primaire pour leur permettre
    de se connecter simultanément.
   </para>

   <para>
    Si vous utilisez une archive WAL, sa taille peut être réduite en utilisant
    l'option <varname>archive_cleanup_command</varname> pour supprimer les fichiers
    qui ne sont plus nécessaires au serveur de standby. Notez toutefois que si vous
    utilisez l'archive à des fins de sauvegarde, vous avez besoin de garder les fichiers
    nécessaires pour restaurer à partir de votre dernière sauvegarde de base, même
    si ces fichiers ne sont plus nécessaires pour le standby.
   </para>
  </sect2>

  <sect2 id="streaming-replication">
   <title>Streaming Replication</title>

   <indexterm zone="high-availability">
    <primary>Streaming Replication</primary>
   </indexterm>

   <para>
    La streaming replication permet à un serveur de standby de rester plus
    à jour qu'il n'est possible avec l'envoi de journaux par fichiers. Le
    standby se connecte au primaire, qui envoie au standby les enregistrements
    de WAL dès qu'ils sont générés, sans attendre qu'un fichier de WAL soit rempli.
   </para>

   <para>
    La streaming replication est asynchrone, il y a donc toujours un petit délai
    entre la validation d'une transaction sur le primaire et le moment où les
    changements sont visibles sur le standby. Le délai est toutefois beaucoup plus petit
    qu'avec l'envoi de fichiers, habituellement en dessous d'une seconde en partant
    de l'hypothèse que le standby est suffisamment puissant pour supporter la charge. Avec
    la streaming replication, <varname>archive_timeout</varname> n'est pas nécessaire
    pour réduire la fenêtre de perte de données.
   </para>

   <para>
    Si vous utilisez la streaming replication sans archivage en continu des fichiers,
    vous devez positionner <varname>wal_keep_segments</varname> sur le maître à une valeur
    suffisamment grande pour garantir que les anciens segments de WAL ne sont pas
    recyclés trop tôt, alors que le standby pourrait toujours avoir besoin d'eux pour
    rattraper son retard. Si le standby prend trop de retard, il aura besoin d'être réinitialisé
    à partir d'une nouvelle sauvegarde de base. Si vous positionnez une archive de WAL qui est accessible
    du standby, wal_keep_segments n'est pas nécessaire, puisque le standby peut toujours
    utiliser l'archive pour rattraper son retard.
   </para>

   <para>
    Pour utiliser la streaming replication, mettez en place un serveur de standby
    en mode fichier comme décrit dans <xref linkend="warm-standby"/>. L'étape qui
    transforme un standby en mode fichier en standby en streaming replication est de
    faire pointer <varname>primary_conninfo</varname> dans le fichier
    <filename>recovery.conf</filename> vers le serveur primaire. Positionnez
    <xref linkend="guc-listen-addresses"/> et les options d'authentification
    (voir <filename>pg_hba.conf</filename>) sur le primaire pour que le serveur
    de standby puisse se connecter à la pseudo-base <literal>replication</literal> 
    sur le serveur primaire (voir <xref linkend="streaming-replication-authentication"/>).
   </para>

   <para>
    Sur les systèmes qui supportent l'option de keepalive sur les sockets, positionner
    <xref linkend="guc-tcp-keepalives-idle"/>,
    <xref linkend="guc-tcp-keepalives-interval"/> et
    <xref linkend="guc-tcp-keepalives-count"/> aide le primaire à reconnaître rapidement
    une connexion interrompue.
   </para>

   <para>
    Positionnez le nombre maximum de connexions concurrentes à partir des
    serveurs de standby (voir <xref linkend="guc-max-wal-senders"/> pour les détails).
   </para>

   <para>
    Quand le standby est démarré et que <varname>primary_conninfo</varname> est
    positionné correctement, le standby se connectera au primaire après avoir
    rejoué tous les fichiers WAL disponibles dans l'archive. Si la connexion
    est établie avec succès, vous verrez un processus walreceiver dans le standby, et
    un processus walsender correspondant sur le primaire.
   </para>

   <sect3 id="streaming-replication-authentication">
    <title>Authentification</title>
    <para>
     Il est très important que les privilèges d'accès pour la réplications soient paramétrés
     pour que seuls les utilisateurs de confiance puissent lire le flux WAL, parce qu'il
     est facile d'en extraire des informations privilégiées. Les serveurs de standby
     doivent s'authentifier sur le primaire avec un compte superutilisateur.
     Par conséquent, un rôle avec les privilèges
     <literal>SUPERUSER</literal> et <literal>LOGIN</literal> doit être créé sur le primaire.
    </para>
    <para>
     L'authentification cliente pour la réplication est contrôlée par un enregistrement de
     <filename>pg_hba.conf</filename> spécifiant <literal>replication</literal> dans le champ
     <replaceable>database</replaceable>. Par exemple, si le standby s'exécute sur un hôte d'IP
     <literal>192.168.1.100</literal>  et que le nom du superutilisateur pour la réplication est
     <literal>foo</literal>, l'administrateur peut ajouter la ligne suivante au fichier
     <filename>pg_hba.conf</filename>  sur le primaire:
     

<programlisting>
# Autoriser l'utilisateur "foo" de l'hôte 192.168.1.100 à se connecter au primaire
# en tant que standby de replication si le mot de passe de l'utilisateur est correctement fourni
#
# TYPE  DATABASE        USER            CIDR-ADDRESS            METHOD
host    replication     foo             192.168.1.100/32        md5
</programlisting>
    </para>
    <para>
     Le nom d'hôte et le numéro de port du primaire, le nom d'utilisateur de la connexion,
     et le mot de passe sont spécifiés dans le fichier <filename>recovery.conf</filename>.
     Le mot de passe peut aussi être enregistré dans le fichier
     <filename>~/.pgpass</filename> sur le serveur en attente (en précisant
     <literal>replication</literal> dans le champ
     <replaceable>database</replaceable>).
     Par exemple, si le primaire s'exécute sur l'hôte d'IP <literal>192.168.1.50</literal>,
     port <literal>5432</literal>, que le nom du superutilisateur pour la réplication est
     <literal>foo</literal>, et que le mot de passe est <literal>foopass</literal>, l'administrateur
     peut ajouter la ligne suivante au fichier <filename>recovery.conf</filename> sur le standby:

<programlisting>
# Le standby se connecte au primaire qui s'exécute sur l'hôte 192.168.1.50
# et port 5432 en tant qu'utilisateur "foo" dont le mot de passe est "foopass"
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
</programlisting>
    </para>
   </sect3>

   <sect3 id="streaming-replication-monitoring">
    <title>Supervision</title>
    <para>
     Un important indicateur de santé de la streaming replication est le nombre
     d'enregistrements générés sur le primaire, mais pas encore appliqués sur
     le standby. Vous pouvez calculer ce retard en comparant le point d'avancement
     des écritures du WAL sur le primaire avec le dernier point d'avancement reçu par
     le standby. Ils peuvent être récupérés en utilisant
     <function>pg_current_xlog_location</function> sur le primaire et
     <function>pg_last_xlog_receive_location</function> sur le standby,
     respectivement (voir <xref linkend="functions-admin-backup-table"/> et
     <xref linkend="functions-recovery-info-table"/> pour plus de détails).
     Le point d'avancement de la réception dans le standby est aussi affiché dans
     le statut du processus de réception des WAL (wal receiver), affiché par
     la commande <command>ps</command> (voyez <xref linkend="monitoring-ps"/> pour plus de détails).
    </para>
   </sect3>

  </sect2>
  </sect1>

  <sect1 id="warm-standby-failover">
   <title>Failover (bascule)</title>

   <para>
    Si le serveur primaire plante alors le serveur de standby devrait commencer
    les procédures de failover.
   </para>

   <para>
    Si le serveur de standby plante alors il n'est pas nécessaire d'effectuer un failover. Si le
    serveur de standby peut être redémarré, même plus tard, alors le processus de récupération
    peut aussi être redémarré au même moment, en bénéficiant du fait que la récupération sait reprendre
    où elle en était. Si le serveur de standby ne peut pas être redémarré, alors
    une nouvelle instance complète de standby devrait être créé.
   </para>

   <para>
    Si le serveur primaire plante, que le serveur de standby devient le 
    nouveau primaire, et que l'ancien primaire redémarre, vous devez avoir
    un mécanisme pour informer l'ancien primaire qu'il n'est plus primaire. C'est aussi
    quelquefois appelé <acronym>STONITH</acronym> (Shoot The Other Node In The Head, ou
    Tire Dans La Tête De L'Autre Noeud), qui est nécessaire pour éviter les situations où
    les deux systèmes pensent qu'ils sont le primaire, ce qui amènerait de la confusion, et
    finalement de la perte de données.
   </para>

   <para>
    Beaucoup de systèmes de failover n'utilisent que deux systèmes, le primaire et le standby,
    connectés par un mécanisme de type ligne de vie (heartbeat) pour vérifier continuellement la
    connexion entre les deux et la viabilité du primaire. Il est aussi
    possible d'utiliser un troisième système (appelé un serveur témoin) pour éviter
    certains cas de bascule inappropriés, mais la complexité supplémentaire
    peut ne pas être justifiée à moins d'être mise en place avec suffisamment
    de précautions et des tests rigoureux.
   </para>

   <para>
    <productname>PostgreSQL</productname> ne fournit pas le logiciel
    système nécessaire pour identifier un incident sur le primaire et notifier
    le serveur de base de standby. De nombreux outils de ce genre existent et sont bien
    intégrés avec les fonctionnalités du système d'exploitation nécessaires à la bascule,
    telles que la migration d'adresse IP.
   </para>

   <para>
    Une fois que la bascule vers le standby se produit, il n'y a plus qu'un
    seul serveur en fonctionnement. C'est ce qu'on appelle un état dégradé.
    L'ancien standby est maintenant le primaire, mais l'ancien primaire est arrêté
    et pourrait rester arrêté. Pour revenir à un fonctionnement normal, un serveur
    de standby doit être recréé,
    soit sur l'ancien système primaire quand il redevient disponible, ou sur un troisième,
    peut être nouveau, système. Une fois que ceci est effectué, le primaire et le standby peuvent
    être considérés comme ayant changé de rôle. Certaines personnes choisissent d'utiliser un troisième
    serveur pour fournir une sauvegarde du nouveau primaire jusqu'à ce que le nouveau serveur de
    standby soit recréé,
    bien que ceci complique visiblement la configuration du système et les procédures d'exploitation.
   </para>

   <para>
    Par conséquent, basculer du primaire vers le serveur de standby peut être rapide mais requiert
    du temps pour re-préparer le cluster de failobver. Une bascule régulière du
    primaire vers le standby est utile, car cela permet une période d'interruption de production sur
    chaque système pour maintenance. Cela vous permet aussi pour vous assurer que 
    votre mécanisme de bascule fonctionnera réellement quand vous en aurez besoin.
		Il est conseillé que les procédures d'administration soient écrites.
   </para>

   <para>
    Pour déclencher le failover d'un serveur de standby en log-shipping, créez un fichier
    trigger (déclencheur) avec le nom de fichier et le chemin spécifiés par le paramètre
    <varname>trigger_file</varname> de <filename>recovery.conf</filename>. Si <varname>trigger_file</varname>
    n'est pas fourni, il n'y a aucun moyen de sortir du mode de recovery sur le standby et de le promouvoir
    en maître. Ceci peut être utile, par exemple, pour des serveurs de reporting qui ne sont
    utilisés que pour décharger le primaire de requêtes en lecture seule, pas à des fins de
    haute disponibilité.
   </para>
  </sect1>

  <sect1 id="log-shipping-alternative">
   <title>Méthode alternative pour le log shipping</title>

   <para>
    Une alternative au mode de standby intégré décrit dans les sections précédentes
    est d'utiliser une <varname>restore_command</varname> qui scrute le dépôt d'archives.
    C'était la seule méthode disponible dans les versions 8.4 et inférieures. Dans cette configuration,
    positionnez <varname>standby_mode</varname>  à off, parce que vous implémentez la scrutation nécessaire
    au fonctionnement standby vous-mêmes. Voyez contrib/pg_standby (<xref linkend="pgstandby"/>) pour
    une implémentation de référence de ceci.
   </para>

   <para>
    Veuillez noter que dans ce mode, le serveur appliquera les WAL fichier par fichier,
    ce qui entraîne que si vous requêtez sur le serveur de standby (voir Hot Standby),
    il y a un délai entre une action sur le maître et le moment où cette action
    devient visible sur le standby, correspondant au temps nécessaire à 
    remplir le fichier de WAL. <varname>archive_timeout</varname>  peut être utilisé pour rendre ce délai
    plus court. Notez aussi que vous ne pouvez combiner la streaming replication avec cette méthode.
   </para>

   <para>
    Les opérations qui se produisent sur le primaire et les serveurs de standby sont
    des opérations normales d'archivage et de recovery. Le seul point de
    contact entre les deux serveurs de bases de données est l'archive de fichiers WAL
    qu'ils partagent: le primaire écrivant dans l'archive, le secondaire
    lisant de l'archive. Des précautions doivent être prises pour s'assurer que les archives WAL de serveurs
    primaires différents ne soient pas mélangées ou confondues. L'archive n'a pas besoin
    d'être de grande taille si elle n'est utilisée que pour le fonctionnement de standby.
   </para>

   <para>
    La magie qui permet aux deux serveurs faiblement couplés de fonctionner ensemble est
    une simple <varname>restore_command</varname> utilisée sur le standby qui
    quand on lui demande le prochain fichier de WAL, attend que le primaire le mette
    à disposition. La <varname>restore_command</varname>  est spécifiée dans le
    fichier <filename>recovery.conf</filename>  sur le serveur de standby. La récupération normale
    demanderait un fichier de l'archive WAL, en retournant un échec si le
    fichier n'était pas disponible. Pour un fonctionnement en standby, il est normal que
    le prochain fichier WAL ne soit pas disponible, ce qui entraîne que le standby doive attendre
    qu'il apparaisse. Pour les fichiers se terminant en <literal>.backup</literal> ou
    <literal>.history</literal> il n'y a pas besoin d'attendre, et un code retour
    différent de zéro doit être retourné. Une <varname>restore_command</varname>  d'attente
    peut être écrite comme un script qui boucle après avoir scruté l'existence du prochain fichier de WAL.
    Il doit aussi y avoir un moyen de déclencher la bascule, qui devrait interrompre la
    <varname>restore_command</varname> , sortir le la boucle et retourner une erreur file-not-found 
    au serveur de standby. Cela met fin à la récupération et le standby démarrera alors comme un serveur normal.
   </para>

   <para>
    Le pseudocode pour une <varname>restore_command</varname> appropriée est:
<programlisting>
triggered = false;
while (!NextWALFileReady() &amp;&amp; !triggered)
{
    sleep(100000L);         /* wait for ~0.1 sec */
    if (CheckForExternalTrigger())
        triggered = true;
}
if (!triggered)
        CopyWALFileForRecovery();
</programlisting>
   </para>

   <para>
    Un exemple fonctionnel de <varname>restore_command</varname> d'attente est fournie
    par le module <filename>contrib</filename>  appelé <application>pg_standby</application>. Il
    devrait être utilisé en tant que référence, comme la bonne façon d'implémenter correctement la logique
    décrite ci-dessus. Il peut aussi être étendu pour supporter des configurations et des 
    environnements spécifiques.
   </para>

   <para>
    La méthode pour déclencher une bascule est une composante importante de la
    planification et de la conception. Une possibilité est d'utiliser la
    commande <varname>restore_command</varname>. Elle est exécutée une fois
    pour chaque fichier WAL, mais le processus exécutant la <varname>restore_command</varname>
    est créé et meurt pour chaque fichier, il n'y a donc ni démon ni processus serveur, et
    on ne peut utiliser ni signaux ni gestionnaire de signaux.  Par conséquent, la
    <varname>restore_command</varname> n'est pas appropriée pour déclencher la bascule.
    Il est possible d'utiliser une simple fonctionnalité de timeout, particulièrement
    si utilisée en conjonction avec un paramètre <varname>archive_timeout</varname>
    sur le primaire. Toutefois, ceci est sujet à erreur, un problème réseau
    ou un serveur primaire chargé pouvant suffire à déclencher une bascule. Un système
    de notification comme la création explicite d'un fichier trigger est idéale, dans la
    mesure du possible.
   </para>

  <sect2 id="warm-standby-config">
   <title>Implémentation</title>

   <para>
    La procédure simplifié pour configurer un serveur de test en utilisant cette
    méthode alternative est la suivante. Pour tous les détails
    sur chaque étape, référez vous aux sections précédentes suivant les indications.
    <orderedlist>
     <listitem>
      <para>
       Paramétrez les systèmes primaire et standby de façon aussi identique que possible,
       y compris deux copies identiques de <productname>PostgreSQL</productname> au même niveau
       de version.
      </para>
     </listitem>
     <listitem>
      <para>
       Activez l'archivage en continu du primaire vers un répertoire d'archives WAL
       sur le serveur de standby. Assurez vous que 
       <xref linkend="guc-archive-mode"/>,
       <xref linkend="guc-archive-command"/> et
       <xref linkend="guc-archive-timeout"/>
       sont positionnés correctement sur le primaire
       (voir <xref linkend="backup-archiving-wal"/>).
      </para>
     </listitem>
     <listitem>
      <para>
       Effectuez une sauvegarde de base du serveur primaire( voir <xref
       linkend="backup-base-backup"/>), , et chargez ces données sur le standby.
      </para>
     </listitem>
     <listitem>
      <para>
       Commencez la récupération sur le serveur de standby à partir de l'archive WAL locale,
       en utilisant un <filename>recovery.conf</filename>  qui spécifie une
       <varname>restore_command</varname>  qui attend comme décrit
       précédemment (voir <xref linkend="backup-pitr-recovery"/>).
      </para>
     </listitem>
    </orderedlist>
   </para>

   <para>
    Le récupération considère l'archive WAL comme étant en lecture seule, donc une fois qu'un fichier WAL
    a été copié sur le système de standby il peut être copié sur bande en même temps
    qu'il est lu par le serveur de bases de données de standby.
    Ainsi, on peut faire fonctionner un serveur de standby pour de la haute disponibilité
    en même temps que les fichiers sont stockés pour de la reprise après sinistre.
   </para>

   <para>
    À des fins de test, il est possible de faire fonctionner le serveur primaire et
    de standby sur le même système. Cela n'apporte rien en termes de robustesse du serveur,
    pas plus que cela ne pourrait être décrit comme de la haute disponibilité.
   </para>
  </sect2>
.
  <sect2 id="warm-standby-record">
   <title>Log Shipping par Enregistrements</title>

   <para>
    Il est aussi possible d'implémenter du log shipping par enregistrements en utilisant
    cette méthode alternative, bien qu'elle nécessite des développements spécifiques,
    et que les modifications ne seront toujours visibles aux requêtes de hot standby qu'après
    que le fichier complet de WAL ait été recopié.
   </para>

   <para>
    Un programme externe peut appeler la fonction <function>pg_xlogfile_name_offset()</function>
    (voir <xref linkend="functions-admin"/>) pour obtenir le nom de fichier et la position exacte
    en octets dans ce fichier de la fin actuelle du WAL. Il peut alors accéder au fichier WAL directement
    et copier les données de la fin précédente connue à la fin courante vers les serveurs de standby.
    Avec cette approche, la fenêtre de perte de données est la période de scrutation du programme de copie,
    qui peut être très petite, et il n'y a pas de bande passante gaspillée en forçant l'archivage 
    de fichiers WAL partiellement remplis. Notez que les scripts <varname>restore_command</varname>
    des serveurs de standby ne peuvent traiter que des fichiers WAL complets, les données copiées
    de façon incrémentale ne sont donc d'ordinaire pas mises à  disposition des serveurs de standby.
    Elles ne sont utiles que si le serveur primaire tombe &mdash; alors le dernier fichier WAL partiel
    est fourni au standby avant de l'autoriser à s'activer. L'implémentation correcte de ce
    mécanisme requiert la coopération entre le script <varname>restore_command</varname> et
    le programme de recopie des données.
   </para>

   <para>
    À partir de <productname>PostgreSQL</productname> version 9.0, vous pouvez utiliser
    la streaming replication (voir <xref linkend="streaming-replication"/>) pour
    bénéficier des mêmes fonctionnalités avec moins d'efforts.
   </para>
  </sect2>
 </sect1>

 <sect1 id="hot-standby">
  <title>Hot Standby</title>

  <indexterm zone="high-availability">
   <primary>Hot Standby</primary>
  </indexterm>

   <para>
    Hot Standby est le terme utilisé pour décrire la possibilité de se
    connecter et d'exécuter des requêtes en lecture seule alors que le
    serveur est en récupération d'archive or standby mode. C'est
    utile à la fois pour la réplication et pour restaurer
    une sauvegarde à un état désiré avec une grande précision.
    Le terme Hot Standby fait aussi référence à la capacité du serveur à passer
    de la récupération au fonctionnement normal tandis-que les utilisateurs
    continuent à exécuter des requêtes et/ou gardent leurs connexions ouvertes.
   </para>

   <para>
    Exécuter des requêtes en mode hot standby est similaire au fonctionnement
    normal des requêtes, bien qu'il y ait quelques différences d'utilisation
    et d'administration notées ci-dessous.
   </para>

  <sect2 id="hot-standby-users">
   <title>Aperçu pour l'utilisateur</title>

   <para>
    Quand le paramètre <xref linkend="guc-hot-standby"/> est configuré à true
    sur un serveur en attente, le serveur commencera à accepter les connexions
    une fois que la restauration est parvenue à un état cohérent. Toutes les
    connexions qui suivront seront des connexions en lecture seule&nbsp;; même
    les tables temporaires ne pourront pas être utilisées.
   </para>

   <para>
    Les données sur le standby mettent un certain temps pour arriver du serveur
    primaire, il y aura donc un délai mesurable entre primaire et standby. La même
    requête exécutée presque simultanément sur le primaire et le standby pourrait par
    conséquent retourner des résultats différents. On dit que la donnée est
    <firstterm>cohérente à terme</firstterm>  avec le primaire. Une fois que
    l'enregistrement de validation (COMMIT) d'une transaction est rejoué sur
    le serveur en attente, les modifications réalisées par cette transaction
    seront visibles par toutes les images de bases obtenues par les transactions
    en cours sur le serveur en attente. Ces images peuvent être prises au début
    de chaque requête ou de chaque transaction, suivant le niveau d'isolation
    des transactions utilisé à ce moment. Pour plus de détails, voir <xref
    linkend="transaction-iso"/>.
   </para>

   <para>
    Les transactions exécutées pendant la période de restauration sur un
    serveur en mode hotstandby peuvent inclure les commandes suivantes&nbsp;:
    <itemizedlist>
     <listitem>
      <para>
       Accès par requête - <command>SELECT</command>, <command>COPY TO</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de curseur - <command>DECLARE</command>, <command>FETCH</command>, <command>CLOSE</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Paramètres - <command>SHOW</command>, <command>SET</command>, <command>RESET</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de gestion de transaction
        <itemizedlist>
         <listitem>
          <para>
           <command>BEGIN</command>, <command>END</command>, <command>ABORT</command>, <command>START TRANSACTION</command>
          </para>
         </listitem>
         <listitem>
          <para>
           <command>SAVEPOINT</command>, <command>RELEASE</command>, <command>ROLLBACK TO SAVEPOINT</command>
          </para>
         </listitem>
         <listitem>
          <para>
           Blocs d'<command>EXCEPTION</command> et autres sous-transactions internes
          </para>
         </listitem>
        </itemizedlist>
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK TABLE</command>, mais seulement quand explicitement dans un de ces modes:
       <literal>ACCESS SHARE</literal>, <literal>ROW SHARE</literal> ou <literal>ROW EXCLUSIVE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Plans et ressources - <command>PREPARE</command>, <command>EXECUTE</command>,
       <command>DEALLOCATE</command>, <command>DISCARD</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Plugins et extensions - <command>LOAD</command>
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Les transactions lancées pendant la restauration d'un serveur en hotstandby
    ne se verront jamais affectées un identifiant de transactions et ne peuvent
    pas être écrites dans les journaux de transactions. Du coup, les actions
    suivantes produiront des messages d'erreur&nbsp;:

    <itemizedlist>
     <listitem>
      <para>
       Langage de Manipulation de Données (LMD ou DML) - <command>INSERT</command>,
       <command>UPDATE</command>, <command>DELETE</command>, <command>COPY FROM</command>,
       <command>TRUNCATE</command>.
       Notez qu'il n'y a pas d'action autorisée qui entraînerait l'exécution d'un 
       trigger pendant la récupération. Cette restriction s'applique même pour
       les tables temporaires car les lignes de ces tables ne peuvent être
       lues et écrites s'il n'est pas possible d'affecter un identifiant de
       transactions, ce qui n'est actuellement pas possible dans un
       environnement Hot Standby.
      </para>
     </listitem>
     <listitem>
      <para>
       Langage de Définition de Données (LDD ou DDL) - <command>CREATE</command>,
       <command>DROP</command>, <command>ALTER</command>, <command>COMMENT</command>.
       Cette restriction s'applique aussi aux tables temporaires car, pour
       mener à bien ces opérations, cela nécessiterait de mettre à jour les
       catalogues systèmes.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>SELECT ... FOR SHARE | UPDATE</command>, car les verrous de
       lignes ne peuvent pas être pris sans mettre à jour les fichiers de
       données.
      </para>
     </listitem>
     <listitem>
      <para>
       Rules sur des ordres <command>SELECT</command> qui génèrent des commandes LMD.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK</command> qui demandent explicitement un mode supérieur à <literal>ROW EXCLUSIVE MODE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK</command> dans sa forme courte par défaut, puisqu'il demande <literal>ACCESS EXCLUSIVE MODE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
      Commandes de gestion de transaction qui positionnent explicitement un état n'étant pas en lecture-seule:
        <itemizedlist>
         <listitem>
          <para>
            <command>BEGIN READ WRITE</command>,
            <command>START TRANSACTION READ WRITE</command>
          </para>
         </listitem>
         <listitem>
          <para>
            <command>SET TRANSACTION READ WRITE</command>,
            <command>SET SESSION CHARACTERISTICS AS TRANSACTION READ WRITE</command>
          </para>
         </listitem>
         <listitem>
          <para>
           <command>SET transaction_read_only = off</command>
          </para>
         </listitem>
        </itemizedlist>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de two-phase commit <command>PREPARE TRANSACTION</command>,
       <command>COMMIT PREPARED</command>, <command>ROLLBACK PREPARED</command>
       parce que même les transactions en lecture seule ont besoin d'écrire dans le WAL
       durant la phase de préparation (la première des deux phases du two-phase commit).
      </para>
     </listitem>
     <listitem>
      <para>
       Mise à jour de séquence - <function>nextval()</function>, <function>setval()</function>
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LISTEN</command>, <command>UNLISTEN</command>, <command>NOTIFY</command>
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Dans le cadre normal, les transactions <quote>en lecture seule</quote>
    permettent la mise à jour des séquences et l'utilisation des instructions
    <command>LISTEN</command>, <command>UNLISTEN</command> et
    <command>NOTIFY</command>, donc les sessions Hot Standby ont des
    restrictions légèrement inférieures à celles de sessions en lecture seule
    ordinaires. Il est possible que certaines des restrictions soient encore
    moins importantes dans une prochaine version.
   </para>

   <para>
    Lors du fonctionnement en serveur hotstandby, le paramètre
    <varname>transaction_read_only</varname> est toujours à true et ne peut
    pas être modifié. Tant qu'il n'y a pas de tentative de modification sur
    la base de données, les connexions sur un serveur en hotstandby se
    comportent de façon pratiquement identiques à celles sur un serveur normal.
    Quand une bascule (<foreignphrase>failover</foreignphrase> ou
    <foreignphrase>switchover</foreignphrase>) survient, la base de données
    bascule dans le mode de traitement normal. Les sessions resteront
    connectées pendant le changement de mode. Quand le mode hotstandby est
    terminé, il sera possible de lancer des transactions en lecture/écriture,
    y compris pour les sessions connectées avant la bascule.
   </para>

   <para>
    Les utilisateurs pourront déterminer si leur session est en lecture seule en
    exécutant <command>SHOW transaction_read_only</command>. De plus, un jeu de
    fonctions (<xref linkend="functions-recovery-info-table"/>) permettent aux utilisateurs d'
    accéder à des informations à propos du serveur de standby. Ceci vous permet d'écrire
    des programmes qui sont conscients de l'état actuel de la base. Vous pouvez
    vous en servir pour superviser l'avancement de la récupération, ou pour écrire des
    programmes complexes qui restaurent la base dans des états particuliers.
   </para>
  </sect2>

  <sect2 id="hot-standby-conflict">
   <title>Gestion des conflits avec les requêtes</title>

   <para>
    Les noeuds primaire et standby sont de bien des façons faiblement couplés. Des
    actions sur le primaire auront un effet sur le standby. Par conséquent, il y a
    un risque d'interactions négatives ou de conflits entre eux. Le conflit le
    plus simple à comprendre est la performance : si un gros chargement de données a
    lieu sur le primaire, il générera un flux similaire d'enregistrements WAL sur le
    standby, et les requêtes du standby pourrait entrer en compétition pour les ressources
    systèmes, comme les entrées-sorties.
   </para>

   <para>
    Il y a aussi d'autres types de conflits qui peuvent se produire avec le
    Hot Standby. Ces conflits sont des <emphasis>conflits durs</emphasis> dans
    le sens où des requêtes pourraient devoir être annulées et, dans certains
    cas, des sessions déconnectées, pour les résoudre. L'utilisateur dispose
    de plusieurs moyens pour gérer ces conflits. Voici les différents cas de
    conflits possibles&nbsp;:

      <itemizedlist>
       <listitem>
        <para>
         Des verrous en accès exclusif pris sur le serveur maître, incluant à
         la fois les commandes <command>LOCK</command> exclusives et quelques
         actions de type <acronym>DDL</acronym>, entrent en conflit avec les
         accès de table des requêtes en lecture seule.
        </para>
       </listitem>
       <listitem>
        <para>
         La suppression d'un tablespace sur le serveur maître entre en conflit
         avec les requêtes sur le serveur standby qui utilisent ce tablespace
         pour les fichiers temporaires.
        </para>
       </listitem>
       <listitem>
        <para>
         La suppression d'une base de données sur le serveur maître entre en
         conflit avec les sessions connectées sur cette base de données sur
         le serveur en attente.
        </para>
       </listitem>
       <listitem>
        <para>
         La copie d'un enregistrement nettoyé par un VACUUM entre en conflit
         avec les transactions sur le serveur en attente qui peuvent toujours
         <quote>voir</quote> au moins une des lignes à supprimer.
        </para>
       </listitem>
       <listitem>
        <para>
         La copie d'un enregistrement nettoyé par un VACUUM entre en conflit
         avec les requêtes accédant à la page cible sur le serveur en attente,
         qu'elles voient ou non les données à supprimer.
        </para>
       </listitem>
      </itemizedlist>
   </para>

   <para>
    Sur le serveur maître, ces cas résultent en une attente
    supplémentaire&nbsp;; l'utilisateur peut choisir d'annuler une des actions
    en conflit. Néanmoins, sur le serveur en attente, il n'y a pas de choix
    possibles&nbsp;: l'action enregistrée dans les journaux de transactions
    est déjà survenue sur le serveur maître et le serveur en standby doit
    absolument réussir à l'appliquer. De plus, permettre que l'enregistrement
    de l'action attende indéfiniment pourrait avoir des effets fortement non
    désirables car le serveur en attente sera de plus en plus en retard par
    rapport au maître. Du coup, un mécanisme est fourni pour forcer
    l'annulation des requêtes sur le serveur en attente qui entreraient en
    conflit avec des enregistrements des journaux de transactions en attente.
   </para>

   <para>
    Voici un exemple de problème type&nbsp;: un administrateur exécute un
    <command>DROP TABLE</command> sur une table du serveur maître qui est
    actuellement utilisé dans des requêtes du serveur en attente. Il est clair
    que la requête ne peut pas continuer à s'exécuter si l'enregistrement
    dans les journaux de transactions, correspondant au <command>DROP
    TABLE</command> est appliqué sur le serveur en attente. Si cette situation
    survient sur le serveur maître, l'instruction <command>DROP TABLE</command>
    attendra jusqu'à ce que l'autre requête se termine. Par contre, quand le
    <command>DROP TABLE</command> est exécuté sur le serveur maître, ce dernier
    ne sait pas les requêtes en cours d'exécution sur le serveur en attente,
    donc il n'attendra pas la fin de l'exécution des requêtes sur le serveur
    en attente. L'enregistrement de cette modification dans les journaux de
    transactions arrivera au serveur en attente alors que la requête sur le
    serveur en attente est toujours en cours d'exécution, causant un conflit.
    Le serveur en attente doit soit retarder l'application des enregistrements
    des journaux de transactions (et tous ceux qui sont après aussi) soit
    annuler la requête en conflit, pour appliquer l'instruction <command>DROP
    TABLE</command>.
   </para>

   <para>
    Quand une requête en conflit est courte, il est généralement préférable
    d'attendre un peu pour l'application du journal de transactions. Mais un
    délai plus long n'est généralement pas souhaitable. Donc, le mécanisme
    d'annulation dans l'application des enregistrements de journaux de
    transactions dispose de deux paramètres, <xref
    linkend="guc-max-standby-archive-delay"/> et <xref
    linkend="guc-max-standby-streaming-delay"/>, qui définissent le délai
    maximum autorisé pour appliquer les enregistrements. Les requêtes en
    conflit seront annulées si l'application des enregistrements prend plus de
    temps que celui défini. Il existe deux paramètres pour que des délais
    différents puissent être observés suivant le cas&nbsp;: lecture des
    enregistrements à partir d'un journal archivé (par exemple lors de la
    restauration initiale à partir d'une sauvegarde ou lors d'un
    <quote>rattrapage</quote> si le serveur en attente accumulait du retard
    par rapport au maître) et lecture des enregistrements à partir de la
    réplication en flux.
   </para>

   <para>
    Pour un serveur en attente dont le but principal est la haute-disponibilité,
    il est préférable de configurer des valeurs assez basses pour les
    paramètres de délai, de façon à ce que le serveur en attente ne soit pas
    trop en retard par rapport au serveur maître à cause des délais suivis à
    cause des requêtes exécutées sur le serveur en attente. Par contre, si
    le serveur en attente doit exécuter des requêtes longues, alors une valeur
    haute, voire infinie, du délai pourrait être préférable. Néanmoins, gardez
    en tête qu'une requête mettant du temps à s'exécuter pourrait empêcher
    les autres requêtes de voir les modifications récentes sur le serveur
    primaire si elle retarde l'application des enregistrements de journaux
    de transactions.
   </para>

   <para>
    La raison la plus fréquente des conflits entre les requêtes en lecture
    seule et le rejeu des journaux de transactions est le <quote>nettoyage
    avancé</quote>. Habituellement, <productname>PostgreSQL</productname>
    permet le nettoyage des anciennes versions de lignes quand aucune
    transaction ne peut les voir pour s'assurer du respect des règles de MVCC.
    Néanmoins, cette règle peut seulement s'appliquer sur les transactions
    exécutées sur le serveur maître. Donc il est possible que le nettoyage
    effectué sur le maître supprime des versions de lignes toujours visibles
    sur une transaction exécutée sur le serveur en attente.
   </para>

   <para>
    Les utilisateurs expérimentés peuvent noter que le nettoyage des versions
    de ligne ainsi que le gel des versions de ligne peuvent potentiellement
    avoir un conflit avec les requêtes exécutées sur le serveur en attente.
    L'exécution d'un <command>VACUUM FREEZE</command> manuel a de grandes
    chances de causer des conflits, y compris sur les tables sans lignes mises
    à jour ou supprimées.
   </para>

   <para>
    Une fois que le délai spécifié par
    <varname>max_standby_archive_delay</varname> ou
    <varname>max_standby_streaming_delay</varname> a été dépassé, toutes les
    requêtes en conflit seront annulées. Ceci résulte habituellement en une
    erreur d'annulation, bien que certains cas, comme un <command>DROP
    DATABASE</command>, peuvent occassionner l'arrêt complet de la connexion.
    De plus, si le conflit intervient sur un verrou détenu par une transaction
    en attente, la session en conflit sera terminée (ce comportement pourrait
    changer dans le futur).
   </para>

   <para>
    Les requêtes annulées peuvent être ré-exécutées immédiatement (après avoir
    commencé une nouvelle transaction, bien sûr). Comme l'annulation des
    requêtes dépend de la nature des enregistrements dans le journal de
    transactions, une requête annulée pourrait très bien réussir si elle est
    de nouveau exécutée.
   </para>

   <para>
    Gardez en tête que les paramètres de délai sont comparés au temps passé
    depuis que la donnée du journal de transactions a été reçue par le serveur
    en attente. Du coup, la période de grâce accordée aux requêtes n'est jamais
    supérieur au paramètre de délai, et peut être considérablement inférieur
    si le serveur en attente est déjà en retard suite à l'attente de la fin
    de l'exécution de requêtes précédentes ou suite à son impossibilité de
    conserver le rythme d'une grosse mise à jour.
   </para>

   <para>
    Les utilisateurs doivent s'attendre à ce que les tables fréquemment mises
    à jour sur le serveur primaire seront aussi fréquemment la cause de
    requêtes annulées sur le serveur en attente. Dans un tel cas, le
    paramétrage d'une valeur finie pour
    <varname>max_standby_archive_delay</varname> ou
    <varname>max_standby_streaming_delay</varname> peut être considéré comme
    similaire à la configuration de <varname>statement_timeout</varname>.
   </para>

   <para>
    Si le nombre d'annulations de requêtes sur le serveur en attente est
    jugé inadmissible, quelques solutions existent. La première option est
    de se connecter au serveur maître et de conserver une requête active assez
    longtemps pour que les requêtes s'exécutent sur le serveur en attente.
    Cela empêche <command>VACUUM</command> de supprimer les versions de lignes
    récemment morts, empêchant du même coup des conflits sur le nettoyage des
    lignes. Cela peut se faire en utilisant <filename>contrib/dblink</filename>
    et <function>pg_sleep()</function>, mais aussi via d'autres mécanismes. Si
    vous le faites, vous devez noter que cela retardera le nettoyage des
    versions de lignes mortes sur le serveur maître, ce qui pourrait résulter
    en une fragmentation non désirée de la table. Néanmoins, cette situation
    ne sera pas meilleure si les requêtes du serveur en attente s'exécutaient
    directement sur le serveur maître. Vous avez toujours le bénéfice de
    l'exécution sur un serveur distant.
    <varname>max_standby_archive_delay</varname> doit être configuré avec une
    valeur suffisamment large dans ce cas car les journaux de transactions
    en retard pourraient déjà contenir des entrées en conflit avec les requêtes
    sur le serveur en attente.
   </para>

   <para>
    Une autre option revient à augmenter <xref
    linkend="guc-vacuum-defer-cleanup-age"/> sur le serveur maître, pour que
    les lignes mortes ne soient pas nettoyées aussi rapidement que d'habitude.
    Cela donnera plus de temps aux requêtes pour s'exécuter avant d'être
    annulées sur le serveur en attente, sans voir à configurer une valeur
    importante de <varname>max_standby_streaming_delay</varname>. Néanmoins,
    il est difficile de garantir une fenêtre spécifique de temps d'exécution
    avec cette approche car <varname>vacuum_defer_cleanup_age</varname> est
    mesuré en nombre de transactions sur le serveur maître.
   </para>
  </sect2>

  <sect2 id="hot-standby-admin">
   <title>Aperçu pour l'administrateur</title>

   <para>
    Si <varname>hot_standby</varname> est positionné à <literal>on</literal> dans
    <filename>postgresql.conf</filename>  et qu'une fichier <filename>recovery.conf</filename>
    est présent, le serveur fonctionnera en mode Hot Standby.
    Toutefois, il pourrait s'écouler du temps avant que les connections en
    Hot Standby soient autorisées, parce que le serveur n'acceptera pas de connexions tant
    que la récupération n'aura pas atteint un point garantissant un état cohérent permettant
    aux requêtes de s'exécuter. Pendant cette période, les clients qui tentent de se connecter
    seront rejetés avec un message d'erreur.
    Pour confirmer que le serveur a démarré, vous pouvez soit tenter de vous connecter en
    boucle, ou rechercher ces messages dans les journaux du serveur:

<programlisting>
LOG:  entering standby mode

... puis, plus loin ...

LOG:  consistent recovery state reached
LOG:  database system is ready to accept read only connections
</programlisting>

    L'information sur la cohérence est enregistrée une fois par checkpoint sur le primaire.
    Il n'est pas possible d'activer le hot standby si on lit des WAL générés durant
    une période pendant laquelle <varname>wal_level</varname> n'était pas positionné
    à <literal>hot_standby</literal> sur le primaire. L'arrivée à un état cohérent
    peut aussi être retardée si ces deux conditions se présentent:

      <itemizedlist>
       <listitem>
        <para>
         Une transaction en écriture a plus de 64 sous-transactions
        </para>
       </listitem>
       <listitem>
        <para>
         Des transactions en écriture ont une durée très importante
        </para>
       </listitem>
      </itemizedlist>

    Si vous effectuez du log shipping par fichier ("warm standby"), vous pourriez
    devoir attendre jusqu'à l'arrivée du prochain fichier de WAL, ce qui pourrait
    être aussi long que le paramètre <varname>archive_timeout</varname> du primaire.
   </para>

   <para>
    Certains paramètres sur le standby vont devoir être revus si ils ont été modifiés
    sur le primaire. Pour ces paramètres, la valeur sur le standby devra
    être égale ou supérieure à celle du primaire. Si ces paramètres ne sont pas
    suffisamment élevés le standby refusera de démarrer. Il est tout à fait possible
    de fournir de nouvelles valeurs plus élevées et de redémarrer le serveur pour reprendre
    la récupération. Ces paramètres sont les suivants:

      <itemizedlist>
       <listitem>
        <para>
         <varname>max_connections</varname>
        </para>
       </listitem>
       <listitem>
        <para>
         <varname>max_prepared_transactions</varname>
        </para>
       </listitem>
       <listitem>
        <para>
         <varname>max_locks_per_transaction</varname>
        </para>
       </listitem>
      </itemizedlist>
   </para>

   <para>
    Il est important que l'administrateur sélectionne le paramétrage approprié
    pour <xref linkend="guc-max-standby-archive-delay"/> et <xref
    linkend="guc-max-standby-streaming-delay"/>. Le meilleur choix varie les
    priorités. Par exemple, si le serveur a comme tâche principale d'être un
    serveur de haute-disponibilité, alors il est préférable d'avoir une
    configuration assez basse, voire à zéro, de ces paramètres. Si le serveur
    en attente est utilisé comme serveur supplémentaire pour des requêtes du
    type décisionnel, il sera acceptable de mettre les paramètres de délai à
    des valeurs allant jusqu'à plusieurs heures, voire même -1 (cette valeur
    signifiant qu'il est possible d'attendre que les requêtes se terminent
    d'elles-même).
   </para>

   <para>
    Les "hint bits" (bits d'indices) écrits sur le primaire ne sont pas journalisés en WAL,
    il est donc probable que les les hint bits soient réécrits sur le standby. Ainsi,
    le serveur de standby fera toujours des écritures disques même si tous les utilisateurs
    sont en lecture seule; aucun changement ne se produira sur les données elles mêmes.
    Les utilisateurs écriront toujours les fichiers temporaires pour les gros tris et
    re-génèreront les fichiers d'information relcache, il n'y a donc pas de morceau de la base
    qui soit réellement en lecture seule en mode hot standby.
    Notez aussi que les écritures dans des bases distantes en utilisant le module
    <application>dblink</application> , et d'autres opération en dehors de la base s'appuyant sur
    des fonctions PL seront toujours possibles, même si la transaction est en lecture seule localement.
   </para>

   <para>
    Les types suivants de commandes administratives ne sont pas acceptées
    durant le mode de récupération:

      <itemizedlist>
       <listitem>
        <para>
         Langage de Définition de Données (LDD ou DDL) - comme <command>CREATE INDEX</command>
        </para>
       </listitem>
       <listitem>
        <para>
         Privilège et possession - <command>GRANT</command>, <command>REVOKE</command>,
         <command>REASSIGN</command>
        </para>
       </listitem>
       <listitem>
        <para>
         Commandes de maintenance - <command>ANALYZE</command>, <command>VACUUM</command>,
         <command>CLUSTER</command>, <command>REINDEX</command>
        </para>
       </listitem>
      </itemizedlist>
   </para>

   <para>
    Notez encore une fois que certaines de ces commandes sont en fait
    autorisées durant les transactions en "lecture seule" sur le primaire.
   </para>

   <para>
    Par conséquent, vous ne pouvez pas créer d'index supplémentaires qui existeraient
    uniquement sur le standby, ni des statistiques qui n'existeraient que sur le standby.
    Si ces commandes administratives sont nécessaires, elles doivent être exécutées
    sur le primaire, et ces modifications se propageront à terme au standby.
   </para>

   <para>
    <function>pg_cancel_backend()</function> fonctionnera sur les processus utilisateurs, mais pas sur
    les processus de démarrage, qui effectuent la récupération. <structname>pg_stat_activity</structname> 
    ne montre pas d'entrée pour le processus de démarrage, et les transactions de récupération
    ne sont pas affichées comme actives. Ainsi, <structname>pg_prepared_xacts</structname> est toujours
    vide durant la récupération. Si vous voulez traiter des transactions préparées douteuses,
    interrogez <structname>pg_prepared_xacts</structname>  sur le primaire, et exécutez les commandes
    pour résoudre le problème à cet endroit.
   </para>

   <para>
    <structname>pg_locks</structname> affichera les verrous possédés par les processus,
    comme en temps normal. <structname>pg_locks</structname> affiche aussi une transaction
    virtuelle gérée par le processus de démarrage qui possède tous les
    <literal>AccessExclusiveLocks</literal> possédés par les transactions rejouées par la récupération.
    Notez que le processus de démarrage n'acquiert pas de verrou pour effectuer les modifications à
    la base, et que par conséquent les verrous autre que <literal>AccessExclusiveLocks</literal> 
    ne sont pas visibles dans <structname>pg_locks</structname> pour le processus de démarrage;
    ils sont simplement censés exister.
   </para>

   <para>
    Le plugin <productname>Nagios</productname> <productname>check_pgsql</productname> fonctionnera,
    parce que les informations simples qu'il vérifie existent.
    Le script de supervision <productname>check_postgres</productname> fonctionnera aussi,
    même si certaines valeurs retournées pourraient être différentes ou sujettes à confusion.
    Par exemple, la date de dernier vacuum ne sera pas mise à jour, puisqu'aucun vacuum ne se déclenche
    sur le standby. Les vacuums s'exécutant sur le primaire envoient toujours leurs modifications
    au standby.
   </para>

   <para>
    Les options de contrôle des fichiers de WAL ne fonctionneront pas durant la récupération,
    comme <function>pg_start_backup</function>, <function>pg_switch_xlog</function>, etc...
   </para>

   <para>
    Les modules à chargement dynamique fonctionnent, comme <structname>pg_stat_statements</structname>.
   </para>

   <para>
    Les verrous consultatifs fonctionnent normalement durant la récupération,
    y compris en ce qui concerne la détection des verrous mortels (deadlocks).
    Notez que les verrous consultatifs ne sont jamais tracés dans les WAL, il est
    donc impossible pour un verrou consultatif sur le primaire ou le standby
    d'être en conflit avec la ré-application des WAL. Pas plus qu'il n'est
    possible d'acquérir un verrou consultatif sur le primaire et que celui-ci
    initie un verrou consultatif similaire sur le standby. Les verrous consultatifs
    n'ont de sens que sur le serveur sur lequel ils sont acquis.
   </para>

   <para>
    Les systèmes de réplications à base de triggers tels que <productname>Slony</productname>,
    <productname>Londiste</productname> et <productname>Bucardo</productname>
    ne fonctionneront pas sur le standby du tout, même s'ils fonctionneront sans problème
    sur le serveur primaire tant que les modifications ne sont pas envoyées sur le serveur standby
    pour y être appliquées. Le rejeu de WAL n'est pas à base de triggers, vous ne pouvez
    donc pas utiliser le standby comme relai vers un système qui aurait besoin d'écritures supplémentaires
    ou utilise des triggers.
   </para>

   <para>
    Il n'est pas possible d'assigner de nouveaux oids, bien que des générateurs d' <acronym>UUID</acronym> 
    puissent tout de même fonctionner, tant qu'ils n'ont pas besoin d'écrire un nouveau statut dans
    la base.
   </para>

   <para>
    À l'heure actuelle, la création de table temporaire n'est pas autorisée durant les
    transactions en lecture seule, certains scripts existants pourraient donc
    ne pas fonctionner correctement. Cette restriction pourrait être levée dans une
    version ultérieure. Il s'agit à la fois d'un problème de respect des standards
    et un problème technique.
   </para>

   <para>
    <command>DROP TABLESPACE</command> ne peut réussir que si le tablespace est vide.
    Certains utilisateurs pourraient utiliser de façon active le tablespace via leur
    paramètre <varname>temp_tablespaces</varname>. S'il y a des fichiers temporaires
    dans le tablespace, toutes les requêtes actives sont annulées pour s'assurer que les
    fichiers temporaires sont supprimés, afin de supprimer le tablespace et de continuer
    l'application des WAL.
   </para>

   <para>
    Exécuter <command>DROP DATABASE</command>, <command>ALTER DATABASE ... SET
    TABLESPACE</command> ou <command>ALTER DATABASE ... RENAME</command> sur
    le serveur maître générera un enregistrement dans les journaux de
    transactions qui causera la déconnexion de tous les utilisateurs
    actuellement connectés à cette base de données. Cette action survient
    immédiatement, quelque soit la valeur du paramètre
    <varname>max_standby_streaming_delay</varname>.
   </para>

   <para>
    En fonctionnement normal (pas en récupération), si vous exécutez 
    <command>DROP USER</command> ou <command>DROP ROLE</command>
    pour un rôle ayant le privilège LOGIN alors que cet utilisateur est toujours
    connecté alors rien ne se produit pour cet utilisateur connecté - il reste connecté. L'utilisateur
    ne peut toutefois pas se reconnecter. Ce comportement est le même en récupération, un
    <command>DROP USER</command> sur le primaire ne déconnecte donc pas cet utilisateur sur le standby.
   </para>

   <para>
    Le collecteur de statistiques est actif durant la récupération. Tous les parcours,
    lectures, utilisations de blocs et d'index, etc... seront enregistrés normalement
    sur le standby. Les actions rejouées ne dupliqueront pas leur effets sur le primaire,
    l'application d'insertions n'incrémentera pas la colonne Inserts de pg_stat_user_tables.
    Le fichier de statistiques est effacé au démarrage de la récupération, les statistiques
    du primaire et du standby différeront donc; c'est vu comme une fonctionnalité, pas un bug.
   </para>

   <para>
    Autovacuum n'est pas actif durant la récupération, il démarrera normalement
    à la fin de la récupération.
   </para>

   <para>
    Le processus d'écriture en arrière plan (background writer) est actif durant
    la récupération et effectuera les restartpoints (points de reprise)
    (similaires aux points de synchronisation ou checkpoints sur le primaire) et
    les activités normales de nettoyage de blocs. Ceci peut inclure la mise à jour
    des information de hint bit des données du serveur de standby.
    La commande <command>CHECKPOINT</command>  est acceptée pendant la récupération,
    bien qu'elle déclenche un restartpoint et non un checkpoint.
   </para>
  </sect2>

  <sect2 id="hot-standby-parameters">
   <title>Référence des paramètres de Hot Standby</title>

   <para>
    De nombreux paramètres ont été mentionnés ci-dessus dans 
    <xref linkend="hot-standby-conflict"/>
    et <xref linkend="hot-standby-admin"/>.
   </para>

   <para>
    Sur le primaire, les paramètres <xref linkend="guc-wal-level"/> et
    <xref linkend="guc-vacuum-defer-cleanup-age"/> peuvent être utilisés.
    <xref linkend="guc-max-standby-archive-delay"/> et <xref
    linkend="guc-max-standby-streaming-delay"/> n'ont aucun effet sur le primaire.
   </para>

   <para>
    Sur le serveur en attente, les paramètres <xref linkend="guc-hot-standby"/>,
    <xref linkend="guc-max-standby-archive-delay"/> et
    <xref linkend="guc-max-standby-streaming-delay"/> peuvent être utilisés.
    <xref linkend="guc-vacuum-defer-cleanup-age"/> n'a pas d'effet tant que
    le serveur reste dans le mode standby, mais deviendra important quand le
    serveur en attente deviendra un serveur maître.
   </para>
  </sect2>

  <sect2 id="hot-standby-caveats">
   <title>Avertissements</title>

   <para>
    Il y a plusieurs limitations de Hot Standby.
    Elles peuvent et seront probablement résolues dans des versions ultérieures:

  <itemizedlist>
   <listitem>
    <para>
     Les opérations sur les index hash ne sont pas écrits dans la WAL à l'heure
     actuelle, la récupération ne mettra donc pas ces index à jour. Les index
     hash ne seront pas utilisés pour les plans d'exécution durant la récupération.
    </para>
   </listitem>
   <listitem>
    <para>
     Une connaissance complète des transactions en cours d'exécution est nécessaire
     avant de pouvoir déclencher des instantanés. Des transactions utilisant un
     grand nombre de sous-transactions (à l'heure actuelle plus de 64) retarderont
     le démarrage des connexions en lecture seule jusqu'à complétion de la plus
     longue transaction en écriture. Si cette situation se produit, des messages
     explicatifs seront envoyés dans la trace du serveur.
    </para>
   </listitem>
   <listitem>
    <para>
     Des points de démarrage valides pour les requêtes de standby sont générés
     à chaque checkpoint sur le maître. Si le standby est éteint alors
     que le maître est déjà éteint, il est tout à fait possible ne ne pas pouvoir
     repasser en Hot Standby tant que le primaire n'aura pas été redémarré, afin
     qu'il génère de nouveaux points de démarrage dans les journaux WAL. Cette situation
     n'est pas un problème dans la plupart des situations où cela pourrait se produire.
     Généralement, si le primaire est éteint et plus disponible, c'est probablement
     en raison d'un problème sérieux qui va de toutes façons forcer la conversion
     du standby en primaire. Et dans des situations où le primaire est éteint
     intentionnellement, la procédure standard est de promouvoir le maître.
    </para>
   </listitem>
   <listitem>
    <para>
     À la fin de la récupération, les <literal>AccessExclusiveLocks</literal> possédés
     par des transactions préparées nécessiteront deux fois le nombre d'entrées normal dans la
     table de verrous. Si vous pensez soit exécuter un grand nombre de transactions préparées
     prenant des <literal>AccessExclusiveLocks</literal>, ou une grosse transaction prenant
     beaucoup de <literal>AccessExclusiveLocks</literal>, il est conseillé d'augmenter la valeur
     de <varname>max_locks_per_transaction</varname>, peut-être jusqu'à une valeur double
     de celle du serveur primaire. Vous n'avez pas besoin de prendre ceci en compte
     si votre paramètre <varname>max_prepared_transactions</varname> est <literal>0</literal>.
    </para>
   </listitem>
   
   
   
   
   
   
   
   
  </itemizedlist>

   </para>
  </sect2>

 </sect1>

  <sect1 id="backup-incremental-updated">
   <title>Sauvegardes mises à jour de façon incrémentielle</title>

  <indexterm zone="high-availability">
   <primary>sauvegardes mises à jour de façon incrémentielle</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>accumulation de modifications</primary>
  </indexterm>

   <para>
    Dans une configuration de standby, il est possible de décharger le serveur 
    primaire du coût des sauvegardes de base périodiques; à la place, les
    sauvegardes de base peuvent être réalisées en sauvegardant les fichiers du
    serveur de standby. Ce concept est habituellement connu sous le nom
    de sauvegardes mises à jour de façon incrémentielle, accumulation de
    modifications de journaux, ou plus simplement, accumulation de modifications.
   </para>

   <para>
    Si nous déclenchons une sauvegarde de système de fichier sur les données
    du serveur de standby alors qu'il traite les journaux reçus du primaire,
    nous pourrons recharger cette sauvegarde et redémarrer le processus de récupération
    à partir du dernier restartpoint. Nous n'avons plus besoin des fichiers de WAL
    d'avant le restartpoint du standby. Si une récupération est nécessaire, il
    sera plus rapide de récupérer à partir de la sauvegarde mise à jour incrémentiellement
    que de la sauvegarde de base original.
   </para>

   <para>
    La procédure pour effectuer une sauvegarde de fichier des données du standby
    alors qu'il traite les journaux provenant du primaire est:
   <orderedlist>
    <listitem>
     <para>
      Effectuer la sauvegarde, sans utiliser <function>pg_start_backup</function> et
      <function>pg_stop_backup</function>. Notez que le fichier <filename>pg_control</filename>
      doit être sauvegardé en <emphasis>premier</emphasis>, comme ici:
      
<programlisting>
cp /var/lib/pgsql/data/global/pg_control /tmp
cp -r /var/lib/pgsql/data /chemin/vers/sauvegarde
mv /tmp/pg_control /chemin/vers/sauvegarde/data/global
</programlisting>
      <filename>pg_control</filename> contient l'endroit où le rejeu du WAL commencera
      après avoir restauré à partir de la sauvegarde; le sauvegarder en premier
      garantit qu'il pointe vers le dernier restartpoint quand la sauvegarde a
      commencé, pas un restartpoint ultérieur au moment où les fichiers ont
      été copiés vers la sauvegarde.
     </para>
    </listitem>
    <listitem>
     <para>
      Notez l'entrée de WAL de la fin de la sauvegarde en appelant
      la fonction <function>pg_last_xlog_replay_location</function> à la
      fin de la sauvegarde, et en la gardant avec la sauvegarde.
<programlisting>
psql -c "select pg_last_xlog_replay_location();" > /chemin/vers/sauvegarde/position_finale
</programlisting>
      Lors d'une récupération à partir d'une sauvegarde mise à jour incrémentiellement,
      le serveur peut accepter les connexions et terminer la récupération avec succès
      avant que la base ne soit devenue cohérente. Pour éviter ceci, vous devez
      vous assurer que la base est cohérente avant que les utilisateurs n'essaient
      de se connecter au serveur et quand la récupération se termine. Vous pouvez le
      faire en comparant l'avancement de la récupération avec la position de WAL finale
      de la sauvegarde que vous avez conservée: le serveur n'est pas cohérent avant
      que la récupération n'ait atteint la position de WAL finale de la sauveqarde.
      L'avancement de la récupération peut aussi être surveillée avec la fonction
      <function>pg_last_xlog_replay_location</function>, mais cela impose de se
      connecter au serveur alors qu'il n'est pas encore cohérent. Il faut donc être
      prudent avec cette méthode.
     </para>
     <para>
     </para>
    </listitem>
   </orderedlist>
   </para>

   <para>
    Puisque le serveur de standby n'est pas <quote>actif</quote>, il n'est pas possible
    d'utiliser <function>pg_start_backup()</function> et<function>pg_stop_backup()</function>
    pour gérer le processus de sauvegarde; ce sera à vous de déterminer quelle quantité de
    fichiers de segments WAL garder pour avoir une sauvegarde récupérable. Ceci est déterminé
    par le dernier restartpoint au moment de la sauvegarde, tout WAL plus ancien que cela
    peut être supprimé de l'archive une fois que la sauvegarde a été réalisée. Vous pouvez
    déterminer le dernier restartpoint en exécutant <application>pg_controldata</application> 
    sur le serveur de standby avant de commencer la sauvegarde, ou en positionnant l'option
    <varname>log_checkpoints</varname> pour écrire les valeurs dans le journal applicatif du
    serveur de standby.
   </para>
  </sect1>

</chapter>
